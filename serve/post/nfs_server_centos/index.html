<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Aravindh.net  | NFS server based on CentOS 7</title>
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">

    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="generator" content="Hugo 0.59.1" />
    
    
      <META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">
    

    
    
      <link href="/dist/css/app.ab4b67a3ea25990fa8279f3b7ef08b61.css" rel="stylesheet">
    

    

    
      
    

    

    <meta property="og:title" content="NFS server based on CentOS 7" />
<meta property="og:description" content="A how-to guide for configuring an NFS server to share binaries for a HPC cluster. Optimized for performance for a read-only scenario." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://aravindh.net/post/nfs_server_centos/" />
<meta property="article:published_time" content="2018-08-07T12:12:29+02:00" />
<meta property="article:modified_time" content="2018-08-07T12:12:29+02:00" />
<meta itemprop="name" content="NFS server based on CentOS 7">
<meta itemprop="description" content="A how-to guide for configuring an NFS server to share binaries for a HPC cluster. Optimized for performance for a read-only scenario.">


<meta itemprop="datePublished" content="2018-08-07T12:12:29&#43;02:00" />
<meta itemprop="dateModified" content="2018-08-07T12:12:29&#43;02:00" />
<meta itemprop="wordCount" content="972">



<meta itemprop="keywords" content="NFS,performance,HPC,sysadmin,CentOS," />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="NFS server based on CentOS 7"/>
<meta name="twitter:description" content="A how-to guide for configuring an NFS server to share binaries for a HPC cluster. Optimized for performance for a read-only scenario."/>

  </head>

  <body class="ma0 avenir bg-near-white">

    
   
      	
        <noscript id="deferred-styles">
            <link rel="stylesheet" type="text/css" href="/css/fonts.css"/>
        </noscript>
        <script>
              var loadDeferredStyles = function() {
                var addStylesNode = document.getElementById("deferred-styles");
                var replacement = document.createElement("div");
                replacement.innerHTML = addStylesNode.textContent;
                document.body.appendChild(replacement)
                addStylesNode.parentElement.removeChild(addStylesNode);
              };
              var raf = window.requestAnimationFrame || window.mozRequestAnimationFrame ||
                  window.webkitRequestAnimationFrame || window.msRequestAnimationFrame;
              if (raf) raf(function() { window.setTimeout(loadDeferredStyles, 0); });
              else window.addEventListener('load', loadDeferredStyles);
        </script>
    	
    	<style>
              
            .Chelsea
            {
                font-family:    'Chelsea',serif;
                font-weight:    400;
            }
             
            .Alegreya
            {
                font-family:    'Alegreya',serif;
                 
            }
            .Alegreya-sans
            {
                font-family:    'Alegreya Sans',serif;
                 
            }
            .Rancho
            {
                font-family:    'Rancho',serif;
            }
        </style>
    	<header class="bg-near-white black-80 tc pv4 avenir">
    		<h1 class="mt2 mb0 Chelsea fw4 f1 black dark-blue b">Aravindh.net</h1>
    		<div>
        		<h2 class="mt2 mb0 f6 fw4 ttu tracked Alegreya-sans">/share/mind</h2>
    		</div>
    		<nav class="bt bb tc mw7 center mt4">
        		<a class="f6 f5-l fw5 link bg-animate black-80 hover-bg-light-green dib pa3 ph4-l Alegreya-sans" href="/">Home</a>
        		<a class="f6 f5-l fw5 link bg-animate black-80 hover-bg-light-green dib pa3 ph4-l Alegreya-sans" href="/about/">About</a>
        		<a class="f6 f5-l fw5 link bg-animate black-80 hover-bg-light-green dib pa3 ph4-l Alegreya-sans" href="/shelf/">Shelf</a>
            <a class="f6 f5-l fw5 link bg-animate black-80 hover-bg-light-green dib pa3 ph4-l Alegreya-sans" href="/wiki/">Wiki</a>
        		<a class="f6 f5-l fw5 link bg-animate black-80 hover-bg-light-green dib pa3 ph4-l Alegreya-sans" href="/projects/">Projects</a>
        		<a class="f6 f5-l fw5 link bg-animate black-80 hover-bg-light-green dib pa3 ph4-l Alegreya-sans" href="/contact/">Contact</a>
   			</nav>
   			<main class="mw6 center pt2">
	    		<article class="dt w-100 b--black-05 pb2 mt2" href="#0">
	    			<div class="dtc w2 w3-ns v-mid">
	        			<img src="/images/authorimage.png" class="ba b--black-10 db br2 w2 w3-ns h2 h3-ns"/>
	      			</div>
	      			<div class="dtc v-mid pl3 Alegreya-sans">
	        			<h1 class="f6 f5-ns fw6 lh-title black mv0 tl">Aravindh Sampathkumar </h1>
	        			<a class="no-underline" href="https://twitter.com/reacharavindh"><h2 class="f6 fw4 mt0 mb0 blue tl">@reacharavindh</h2></a>
	        			<h2 class="f6 fw6 lh-title black-70 mv0 tl"> Performance Engineer and Sysadmin | Ex @NetApp </h2>
	        			<h2 class="f6 fw4 mt0 mb0 black-60 tl">Jack of many trades and aspiring master of some, all around good guy to have on your team.</h2>
	      			</div>
	    		</article>
			</main>
		</header>

    <main class="pb3" role="main">
      


<article class="mw8 mw8-ns center bg-near-white">
  <div class="flex flex-column flex-row-ns">
    <div class="pa2 pa0-ns pr3-ns mb2 mb0-ns w-100 w-30-ns">
      <img src="/images/NFS_server.jpg" class="db" alt="Featured image of this post">
    </div>
    <div class="w-100 w-70-ns mw6-ns pl2 pl3-ns">
      <a class="no-underline black" href="/post/nfs_server_centos/">
        <h1 class="f2 fw5 Alegreya mb1">NFS server based on CentOS 7</h1>
        <time class="f6 mv1 dib tracked Alegreya-sans" datetime="2018-08-07T12:12:29&#43;02:00">August 7, 2018</time>
        <p class="f6 lh-copy mv0 Alegreya-sans">By Aravindh Sampathkumar</p>
      </a>
    </div>
  </div>
</article>

<main class="mw6 mw8-ns center bg-near-white">
  <div class="flex flex-column flex-row-ns">
    <div class="pa2 pa0-ns mt4-ns w-100 w-70-ns f6 f5-ns Alegreya lh-copy center">

<p><style>
p {text-indent: 8%;}
</style></p>

<h3 id="nfs">NFS</h3>

<p><strong>N</strong>etwork <strong>F</strong>ile <strong>S</strong>ystem(NFS) is a widely used distributed filesystem that works reasonably well in most scenarios where you must share a filesytem across several computers. It can use UDP or TCP underneath for transport, and is ofcourse very much configurable to get the best out of it for specific usecases.</p>

<p>At our research cluster, we use NFS for making user&rsquo;s home directories and research datasets available on all compute nodes. For such a serious usecase, the NFS server needs to be sophisticated enough with caches at several layers to stand up for the performance and durability needs of the workload. So, we rely on an enterprise grade <strong>N</strong>etwork <strong>A</strong>ttached <strong>S</strong>torage system - Isilon for an optimized NFS server.</p>

<h3 id="context">Context</h3>

<p>What I&rsquo;m dealing with today, is to share a filesystem with software binaries across all compute nodes in the cluster in a read-only manner. Basically, I will install a software package on one server, and copy the binaries to a location, and voila! it is ready for use by all compute nodes instantly.
<div class="center mw6-ns">
<img class="pt0 w-100 center" src="/images/read-only-nfs.png" alt="Read only mounts of NFS"></img>
</div></p>

<p>My goal is to setup in such a way that clients mount this filesystem in read-only mode, and cache it heavily for local use. I, as an administrator will have a way to purge this client-side cache in an automated way when I update the binaries on the NFS server.</p>

<h3 id="server-side-setup">Server-side setup</h3>

<p>As a good first step, update the server and grab the latest NFS utilities.</p>

<pre><code class="language-bash">yum -y update
yum -y install nfs-utils
</code></pre>

<p>Add our domain name to idmapd.</p>

<pre><code class="language-bash">echo &quot;Domain = nice.domain.name&quot; &gt;&gt; /etc/idmapd.conf
</code></pre>

<p>I want to export /opt on the NFS server to be available for all compute nodes in the cluster.</p>

<pre><code class="language-bash">echo &quot;/opt 172.16.0.0/17(ro)&quot; &gt;&gt; /etc/exports
</code></pre>

<p>To learn more about export options, check the man page..</p>

<pre><code class="language-bash">man 5 exports
</code></pre>

<p>Let&rsquo;s tell systemd to start our NFS server and start it at boot everytime.</p>

<pre><code class="language-bash">systemctl start rpcbind nfs-server
systemctl enable rpcbind nfs-server
</code></pre>

<p>Add permissions through the firewall so that our clients can talk to this server.</p>

<pre><code class="language-bash">firewall-cmd --add-service=nfs --permanent 
firewall-cmd --reload
</code></pre>

<p>Let&rsquo;s add a test file to later verify it on the client.</p>

<pre><code class="language-bash">echo &quot;Success if you see this&quot; &gt; /opt/testfile
</code></pre>

<h4 id="performance-notes">Performance notes:</h4>

<p><div class="dt center pb1">
  <div class="db dtc-ns v-mid-ns">
    <img src="/images/threads.jpeg" alt="NFS threads" class="w-100 mw7 w5-ns" />
  </div>
  <div class="db dtc-ns v-mid ph2 pr0-ns pl3 pl4-ns">
    <p class="lh-copy">
      Let&rsquo;s talk threads. If you&rsquo;re going for maximum utilisation of the server resources for this NFS server, you should tell NFS to start atleast as many daemon threads as many CPU cores available in the system. In my case, my server has 8 cores. The default # of threads with NFS on Centos 7 is 8. However, I expect this NFS server to concurrently serve about 100 compute nodes when they use software binaries from /opt. So, I could use the extra bit of concurrency.
    </p>
  </div>
</div>
It can be checked at</p>

<pre><code class="language-bash">grep 'RPCNFSDCOUNT=' /etc/sysconfig/nfs
RPCNFSDCOUNT=16
</code></pre>

<p>I modified it to 16 to have 16 NFS threads servicing clients. I will increase this number in future if real workloads show a need for it.</p>

<p>There are other knobs to turn on both server and client to adapt to the kind of workload that is expected, but those are out of scope of this post.
If you&rsquo;re running a rw workload over NFS and have a lot of time to spare to understand NFS caching and squeezing every last bit of performance out of your setup, the following references would help..</p>

<ol>
<li><a href="https://helpful.knobs-dials.com/index.php/NFS_notes">https://helpful.knobs-dials.com/index.php/NFS_notes</a></li>
<li><a href="https://docstore.mik.ua/orelly/networking_2ndEd/nfs/ch07_04.htm">https://docstore.mik.ua/orelly/networking_2ndEd/nfs/ch07_04.htm</a></li>
<li><a href="https://www.avidandrew.com/understanding-nfs-caching.html">https://www.avidandrew.com/understanding-nfs-caching.html</a></li>
<li><a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/storage_administration_guide/ch-nfs">https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/storage_administration_guide/ch-nfs</a></li>
<li><a href="http://www.admin-magazine.com/HPC/Articles/Useful-NFS-Options-for-Tuning-and-Management">http://www.admin-magazine.com/HPC/Articles/Useful-NFS-Options-for-Tuning-and-Management</a></li>
</ol>

<p>In my case, since my only focus is on servicing reads. I optimized my NFS server with bigger system read buffers so that the NFS daemon threads can service reads faster from memory. The following tweaks the configuration in a way that buffer sizes start from 256 KiB and can be increased to 512 KiB upon need.</p>

<pre><code class="language-bash">echo &quot;net.core.rmem_default = 262144&quot; &gt;&gt; /etc/sysctl.conf
echo &quot;net.core.rmem_max = 524288&quot; &gt;&gt; /etc/sysctl.conf
sysctl -p
</code></pre>

<p>Note: I did this based on my experience from several workloads. Test these kind of knobs with your workloads before turning knobs.</p>

<h3 id="client-side-setup">Client-side setup</h3>

<p>As a good first step, update the system and grab the latest NFS utilities.</p>

<pre><code class="language-bash">yum -y update
yum -y install nfs-utils
</code></pre>

<p>Add our domain name to idmapd.</p>

<pre><code class="language-bash">echo &quot;Domain = nice.domain.name&quot; &gt;&gt; /etc/idmapd.conf
</code></pre>

<p>Enable rpcbind to let the NFS client to talk to the NFS server,</p>

<pre><code class="language-bash">systemctl start rpcbind 
systemctl enable rpcbind
</code></pre>

<p>Mount the filesystem for testing by</p>

<pre><code class="language-bash">mount -t nfs -o 'ro' nice.domain.name:/opt /opt
</code></pre>

<p>Now, let&rsquo;s see if we are able to read the test file we created on the server.</p>

<pre><code class="language-bash">&gt; cat /opt/testfile 
Success if you see this
</code></pre>

<p>Great, things work as we expected. Let&rsquo;s make it permanent and happen at boot time.
In simple environments, you can add an entry to /etc/fstab.</p>

<pre><code class="language-bash">echo &quot;nice.domain.name:/opt  /opt                   nfs     defaults        0 0&quot; &gt;&gt; /etc/fstab
</code></pre>

<p>However, I make use of autofs in my environment. In my specific case, the file /etc/autofs.centos tells the kernel where the mount locations are. The following does it for me.</p>

<pre><code class="language-bash">echo &quot;/opt                            -noatime,nodiratime,ro opt.ghpc.au.dk:/opt&quot; &gt;&gt; /etc/autofs.centos
systemctl reload autofs.service
</code></pre>

<p>Let&rsquo;s see what is mounted..</p>

<pre><code class="language-bash">&gt; cat /proc/mounts|grep opt
/etc/auto.centos.ghpc /opt autofs rw,relatime,fd=19,pgrp=26422,timeout=300,minproto=5,maxproto=5,direct,pipe_ino=1272693 0 0

opt.ghpc.au.dk:/opt /opt nfs4 ro,noatime,vers=4.1,rsize=1048576,wsize=1048576,namlen=255,hard,proto=tcp,port=0,timeo=600,retrans=2,sec=sys,clientaddr=172.16.2.30,local_lock=none,addr=172.16.2.37 0 0
</code></pre>

<p>As we can see,
Our mount requested has negotiated NFS V4.1 with the server, set itself up as read-only, and uses the rest as defaults.
When running amongst fairly recent Linux kernels, it is better to go with NFS 4.1 for performance.
For the curious, here are a couple of studies about NFSv4.1 performance</p>

<ol>
<li><a href="https://www.fsl.cs.sunysb.edu/docs/nfs4perf/nfs4perf-sigm15.pdf">Paper - An Evaluation of NFSv4.1</a></li>
<li><a href="https://www.fsl.cs.sunysb.edu/docs/nfs4perf/nfs4perf-microscope.pdf">Paper - NFSv4.1 Performance under a microscope</a></li>
</ol>

<p>However, it is not always true that NFSv4.1 is faster than NFSv3. Some enterprise vendors&rsquo; implementation of the NFSv4.1 protocol are still immature, and they would recommend you to stay with NFSv3. Obey their recommendations unless you want a huge performance hit.</p>

<h3 id="conclusion">Conclusion</h3>

<p>Without any surprises, we were able to setup a simple NFS client &amp; server. The client setup can be replicated in many other compute nodes to share the file system.</p>
</div>
    <div class="w-100 w-30-ns pl1 pl4-ns mt4-ns"><div class="bg-light-gray pa3 nested-list-reset nested-copy-line-height nested-links Alegreya">
    <p class="f5 b mb3">What's in this Post</p>
      <nav id="TableOfContents">
<ul>
<li>
<ul>
<li>
<ul>
<li><a href="#nfs">NFS</a></li>
<li><a href="#context">Context</a></li>
<li><a href="#server-side-setup">Server-side setup</a>
<ul>
<li><a href="#performance-notes">Performance notes:</a></li>
</ul></li>
<li><a href="#client-side-setup">Client-side setup</a></li>
<li><a href="#conclusion">Conclusion</a></li>
</ul></li>
</ul></li>
</ul>
</nav>
  </div>




  <div class="bg-light-gray pa3 nested-list-reset nested-copy-line-height nested-links Alegreya">
    <p class="f5 b mb3">Related</p>
    <ul class="pa0 list">
	   
	     <li  class="mb2">
          <a href="/post/zfs_performance/">ZFS performance</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/post/zfs_fileserver/">ZFS file server</a>
        </li>
	    
    </ul>
</div>

</div>

</main>

    </main>
    <footer class="pv1 ph3 ph5-m ph6-l bg-black">
	<div class="tc">
    	<a class="light-gray f3 f2-ns fw5 tc link dim dib mt4 mb4 mb0-l Alegreya-sans" href="mailto:aravindh@fastmail.com" >
        	aravindh@fastmail.com
    	</a>
  </div>
  <div class="tc mt3 mb6">
    	<a class="link near-white hover-blue dib h2 w2 mr3" href="https://github.com/aravindhsampath" title="GitHub">
      		<svg fill="currentColor" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" fill-rule="evenodd" clip-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="1.414"><path d="M8 0C3.58 0 0 3.582 0 8c0 3.535 2.292 6.533 5.47 7.59.4.075.547-.172.547-.385 0-.19-.007-.693-.01-1.36-2.226.483-2.695-1.073-2.695-1.073-.364-.924-.89-1.17-.89-1.17-.725-.496.056-.486.056-.486.803.056 1.225.824 1.225.824.714 1.223 1.873.87 2.33.665.072-.517.278-.87.507-1.07-1.777-.2-3.644-.888-3.644-3.953 0-.873.31-1.587.823-2.147-.083-.202-.358-1.015.077-2.117 0 0 .672-.215 2.2.82.638-.178 1.323-.266 2.003-.27.68.004 1.364.092 2.003.27 1.527-1.035 2.198-.82 2.198-.82.437 1.102.163 1.915.08 2.117.513.56.823 1.274.823 2.147 0 3.073-1.87 3.75-3.653 3.947.287.246.543.735.543 1.48 0 1.07-.01 1.933-.01 2.195 0 .215.144.463.55.385C13.71 14.53 16 11.534 16 8c0-4.418-3.582-8-8-8"/></svg>
    	</a>
    	<a href="https://www.facebook.com/aravindh.sathish" class="link dim dib mr3 near-white" title="Impossible Labs on Facebook">
        	<svg class="db w2 h2" data-icon="facebook" viewBox="0 0 32 32" fill="currentColor">
          	<title >facebook icon</title>
          	<path d="M8 12 L13 12 L13 8 C13 2 17 1 24 2 L24 7 C20 7 19 7 19 10 L19 12 L24 12 L23 18 L19 18 L19 30 L13 30 L13 18 L8 18 z">	
          		</path>
        	</svg>
    	</a>
    	<a href="https://twitter.com/reacharavindh" class="link dim dib mr3 near-white">
        	<svg class="db w2 h2" data-icon="twitter" viewBox="0 0 32 32"
          		fill="currentColor" >
          		<title >twitter icon</title>
          		<path d="M2 4 C6 8 10 12 15 11 A6 6 0 0 1 22 4 A6 6 0 0 1 26 6 A8 8 0 0 0 31 4 A8 8 0 0 1 28 8 A8 8 0 0 0 32 7 A8 8 0 0 1 28 11 A18 18 0 0 1 10 30 A18 18 0 0 1 0 27 A12 12 0 0 0 8 24 A8 8 0 0 1 3 20 A8 8 0 0 0 6 19.5 A8 8 0 0 1 0 12 A8 8 0 0 0 3 13 A8 8 0 0 1 2 4">
          		</path>
        	</svg>
    	</a>
    	<a href="https://www.linkedin.com/aravindhsampath" class="link dim dib black-30">
        	<svg class="db w2 h2" x="0px" y="0px" viewBox="0 0 48 48" >
          		<linearGradient gradientUnits="userSpaceOnUse" x1="23.9995"
            		y1="0" x2="23.9995" y2="48.0005" >
            		<stop offset="0" ></stop>
            		<stop offset="1" ></stop>
          		</linearGradient>
          		<path fill="currentColor" d="M48,42c0,3.313-2.687,6-6,6H6c-3.313,0-6-2.687-6-6V6 c0-3.313,2.687-6,6-6h36c3.313,0,6,2.687,6,6V42z">
          		</path>
          		<g >
            		<g >
              			<path fill="#FFFFFF" d="M15.731,11.633c-1.608,0-2.658,1.083-2.625,2.527c-0.033,1.378,1.018,2.494,2.593,2.494 c1.641,0,2.691-1.116,2.691-2.494C18.357,12.716,17.339,11.633,15.731,11.633z M13.237,35.557h4.988V18.508h-4.988V35.557z M31.712,18.748c-1.595,0-3.222-0.329-4.956,2.36h-0.099l-0.087-2.599h-4.417c0.065,1.411,0.074,3.518,0.074,5.52v11.529h4.988 v-9.854c0-0.46,0.065-0.919,0.196-1.248c0.328-0.919,1.149-1.871,2.527-1.871c1.805,0,2.527,1.411,2.527,3.479v9.494h4.988V25.439 C37.455,20.713,34.993,18.748,31.712,18.748z">
              			</path>
            		</g>
         		</g>
        	</svg>
    	</a>
  	</div>

</footer>
</body>
</html>

    

  <script async src="/dist/js/app.3fc0f988d21662902933.js"></script>


  </body>
</html>
