[{"content":" Welcome to my digital garden. I write about computers, my wood working adventures and anything that captures my fascination enough to obsess over for a time.\n","date":null,"permalink":"/","section":"Aravindh - SRE","summary":"Welcome to my digital garden.","title":"Aravindh - SRE"},{"content":"","date":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"I’m trying to learn Nederlands with a goal of being able to use it conversationally in practical life atleast at a child level. This is my evolving set of notes.\nJe praat en schrijft over het opknappen en inrichten van je huis.\nYou talk and write about renovating and furnishing your home.\nJe leest, schrijft en praat over contact zoeken in de buurt.\nYou read, write and talk about making contact in the neighborhood.\nJe praat en schrijft over reparaties in je huis.\nYou talk and write about repairs in your home.\nJe leest, praat en schrijft over verschillende woonplaatsen.\nYou read, talk and write about different places to live.\nGefeliciteerd met je nieuwe huis! Maar wat nu? Je moet het huis misschien nog opknappen. En je wilt het natuurlijk leuk inrichten. Hoe doe je dat als je niet zoveel geld hebt? En wat mag je in je huis veranderen? Op deze website vind je informatie en tips.\nCongratulations on your new home! But what now? You may still have to renovate the house. And of course you want to decorate it nicely. How do you do that if you don\u0026rsquo;t have much money? And what can you change in your home? On this website you will find information and tips.\nHeb je een koophuis? Dan mag je alles aan de binnenkant van het huis veranderen. In een huurwoning is dat anders. Je mag bijvoorbeeld wel verven en behangen, maarje mag een huurhuis niet zomaar verbouwen. Vraag toestemming aan de eigenaar, als je bijvoorbeeld een nieuwe badkamer wilt.\nDo you own a home? Then you can change everything on the inside of the house. In a rental home this is different. For example, you are allowed to paint and wallpaper, but you are not allowed to simply renovate a rental home. Ask the owner for permission, for example if you want a new bathroom.\nAls het huis door de verbouwing in waarde omhooggaat, betaalt de eigenaar soms mee. Daarsta je in je nieuwe huis. Wat ga je doen? Controleer eerst het hele huis. Zitten er scheuren en gaten in de muren? Sluiten de ramen en deuren goed? Moet je een nieuwe vloer leggen? Moet je muren verven of behangen? Moet je apparaten in de keuken vervangen? Is er een televisie- en internetaansluiting? Moet je nog ergens plankjes ophangen?\nIf the house increases in value due to the renovation, the owner sometimes pays for it. There you are in your new home. What are you going to do? First check the entire house. Are there cracks and holes in the walls? Do the windows and doors close properly? Do you need to lay a new floor? Do you need to paint or wallpaper walls? Do you need to replace appliances in the kitchen? Is there a television and internet connection? Do you need to hang shelves somewhere?\nJe weet nu wat je moet doen. Hopelijk ben je zelf handig of heb je handige vrienden en familie. Je kunt veel geld besparen als je het huis zelf opknapt. En denk ook eens aan spullen huren. Duur gereedschap kun je huren, bij winkels zoals Gamma. Echt grote klussen kan een vakman meestal beter doen.\nYou now know what to do. Hopefully you are handy yourself or have handy friends and family. You can save a lot of money if you renovate the house yourself. And also consider renting things. You can rent expensive tools from shops such as Gamma. Really big jobs are usually better done by a professional.\nVergeet ondertussen niet dat je ook geld voor de inrichting moet overhouden. Heb je nieuwe gordijnen nodig? Denk ook aan de verlichting. Moet je nieuwe lampen kopen? Passen je meubels in het nieuwe huis? Maak een lijstje van dingen die je moet kopen. Let op de aanbiedingen bij winkels zoals LeenBakker en Ikea. En kijk ook eens bij de kringloopwinkel of kijk op internet, op Marktplaats. Een tweedehands bank of tafel kan heel mooi zijn en is veel voordeliger dan nieuw natuurlijk.\nIn the meantime, don\u0026rsquo;t forget that you also need to save money for the interior. Do you need new curtains? Also think about the lighting. Do you need to buy new lamps? Does your furniture fit in the new house? Make a list of things you need to buy. Pay attention to the offers at shops such as LeenBakker and Ikea. And also take a look at the thrift store or look on the internet, at Marktplaats. A second-hand sofa or table can be very nice and is of course much cheaper than new.\nToen ik net in Nederland woonde, kende ik niemand in mijn straat. Ik kon geen contact maken, omdat ik nog geen Nederlands sprak. En niemand probeerde met mij contact te maken. Ik miste mijn familie en vrienden uit Iran. Ik voelde me eenzaam.\nWhen I first moved to the Netherlands, I didn\u0026rsquo;t know anyone in my street. I couldn\u0026rsquo;t make contact because I didn\u0026rsquo;t speak Dutch yet. And no one tried to make contact with me. I missed my family and friends from Iran. I felt lonely.\n**Maar op een dag stond Marja voor mijn deur. Ze woonde aan de overkant van de straat. Ze gaf me een briefje met een vertaling in mijn eigen taal:\n\u0026lsquo;Wil je Nederlands leren?\u0026quot;. Dat vond ik heel lief.**\nBut one day, Marja stood at my door. She lived across the street. She gave me a note with a translation in my own language: \u0026lsquo;Do you want to learn Dutch?\u0026rsquo; I thought that was very sweet.\nVanaf dat moment is Marja mijn taalmaatje. Ze is alleenstaand, net als ik. We zien elkaar één keer per week. Dan praten we bijvoorbeeld over haar kleinkinderen of de verschillen tussen onze culturen. Koken is onze gezamenlijke hobby. We koken samen typisch Nederlandse en Iraanse gerechten. Marja heeft gewerkt als docent, maar nu is ze met pensioen. Zelf wil ik graag een opleiding voor verpleegkundige volgen, maar eerst moet ik Staatsexamen doen.\nFrom that moment on, Marja has been my language buddy. She is single, just like me. We see each other once a week. Then we talk about her grandchildren or the differences between our cultures, for example. Cooking is our shared hobby. We cook typical Dutch and Iranian dishes together. Marja used to work as a teacher, but she is now retired. I would like to study to become a nurse myself, but first I have to take the State Exam.\nDoor de gesprekken met Marja spreek ik al veel beter Nederlands. Vroeger schaamde ik me soms als ik met Nederlanders praatte. Nu vind ik het leuk om een praatje te maken.\u0026rsquo;\nThanks to the conversations with Marja, I already speak Dutch much better. In the past, I sometimes felt ashamed when talking to Dutch people. Now I enjoy having a chat.\nBasic grammar notes #omdat construction - verb always comes at the end of the sentence. Ik kon geen contact maken, omdat ik nog geen Nederlands sprak. I couldn\u0026rsquo;t make contact because I didn\u0026rsquo;t speak Dutch yet.\nVocabulary #over - about\nhet opknappen - renovating\ninrichten - decorating\nzoeken - finding/searching\nverschillende - various\nwoonplaatsen - places to live\nveranderen - change\nbinnenkant - inside\nhuurwoning - rental house\nbijvoorbeeld - example\nverven - to paint\ntoestemming - permission\nsoms - sometimes\nbetaalt - pays\nomhooggaat - increases\nControleer - check\nergens - somewhere\nbesparen - save (geld besparen)\nzoals - such as\nEcht grote - Really big\nmeestal - usually\nlijstje van dingen - list of things\naanbiedingen - offers\nvoordeliger - cheaper\nnet - just\nToen - when\nkende - knew\nNiemand - nobody\nkon - could\nkon geen - couldnt\nvoelde - felt\neenzaam - lonely\ngaf - gave\nvertaling - translation\nelkaar - each other\ntussen - between\ngezamenlijke - shared\nopleiding - education\nverpleegkundige - nurse\nVroeger - In the past\nschaamde - ashamed\nCommon phrases #Maar wat nu?\nWat ga je doen? - What are you going to do?\nJe moet het huis misschien nog opknappen - You may still have to X\nwat mag je in je huis veranderen? - What can you [ in X ] change?\nHeb je een X? - Do you have a X?\nVraag toestemming aan de eigenaar - Ask permission from X\nJe weet nu wat je moet doen. - You know what to do.\nHeb je nieuwe gordijnen nodig? - Do you need new curtains?\nEn kijk ook eens bij de - And also take a look at the..\nToen ik net in Nederland woonde - When I first moved to Netherlands\u0026hellip;\nIk voelde me eenzaam. - I felt lonely - I felt X\nZe gaf me een briefje - She gave me a note. She gave me X\nVanaf dat moment - From that moment….\nnet als ik - just like me.\n","date":"28 November 2024","permalink":"/posts/notes-nl/","section":"Posts","summary":"Notes from Learning Dutch - chapter 1","title":"Chapter 1 - Notes from Learning Dutch"},{"content":"","date":null,"permalink":"/tags/dutch/","section":"Tags","summary":"","title":"Dutch"},{"content":"","date":null,"permalink":"/categories/dutch/","section":"Categories","summary":"","title":"Dutch"},{"content":"","date":null,"permalink":"/tags/llm/","section":"Tags","summary":"","title":"LLM"},{"content":"","date":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts"},{"content":"","date":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"","date":null,"permalink":"/tags/cicd/","section":"Tags","summary":"","title":"Cicd"},{"content":"","date":null,"permalink":"/categories/computers/","section":"Categories","summary":"","title":"Computers"},{"content":"","date":null,"permalink":"/tags/hugo/","section":"Tags","summary":"","title":"Hugo"},{"content":"Goal #Have a seamless way to write blog posts on my Mac and have the website transparently build and deploy upon saving the file locally. This means that whenever I make changes to my website\u0026rsquo;s code, the updates are automatically deployed to production without any manual intervention.\nHow? # Noteshub.app #Finally! An easy to use (local) Markdown editor that directly works with Github repository, as in transparently saves your files as a commit to the repository upon saving. Bonus - It has excalidraw built-in for making scgs like above.\nNoteshub\nGithub Actions #Here is a simple Github Actions workflow that does the following:\nClone the contents of the repository Install hugo Build the static website using hugo Commit the newly built website to the same github repo name: Build and Deploy Hugo Website on: push: branches: - master jobs: build-and-deploy: runs-on: ubuntu-latest steps: - name: Checkout code uses: actions/checkout@v2 - name: Install dependencies run: | sudo apt-get update \u0026amp;\u0026amp; \\ sudo apt-get install -y hugo - name: Build website env: HUGO_ENV: production run: | hugo --minify - name: Commit and push changes uses: devops-infra/action-commit-push@master with: github_token: ${{ secrets.COMMIT_TOKEN }} commit_message: hugo_build_by_Actions Deployer.sh #Here is a simple script I run on my Linux VM(Hetzner, but any infrastructure provider would do just the same). The script polls github for the latest commit tag and if it is different from the local repo, does a git pull and deploy the changes to the web server root.\n#!/usr/bin/bash # Set the URL of the GitHub repository GITHUB_REPO_URL=\u0026#34;https://github.com/aravindhsampath/aravindh.net\u0026#34; # Set the path to the local clone of the repository LOCAL_REPO_PATH=\u0026#34;/home/asampath/aravindh.net\u0026#34; # Set the path to the web server\u0026#39;s document root WEB_SERVER_DOC_ROOT=\u0026#34;/home/caddy/www\u0026#34; # Check if the repository has changed since the last check if [ -f \u0026#34;$LOCAL_REPO_PATH/.git/HEAD\u0026#34; ]; then # Get the latest commit hash from the GitHub repository GITHUB_COMMIT_HASH=$(curl -s -X GET \u0026#34;https://api.github.com/repos/aravindhsampath/aravindh.net/commits?per_page=1\u0026#34; | jq -r \u0026#39;.[0].sha\u0026#39;) cd $LOCAL_REPO_PATH # Check if the local repository is up-to-date with the remote repository if [ \u0026#34;$(git rev-parse HEAD)\u0026#34; != \u0026#34;$GITHUB_COMMIT_HASH\u0026#34; ]; then # Update the local repository to match the latest commit hash from GitHub /usr/bin/git pull origin master --rebase # Deploy changes to the web server\u0026#39;s document root /usr/bin/rsync -avh $LOCAL_REPO_PATH/public/ /home/caddy/www/ fi fi And this script is run on cron as such:\n*/15 9-23 * * * asampath /usr/bin/bash /home/asampath/deployer.sh The script polls for changes in the github repository every 15 minutes and “deploys” the website if there are any changes. The result is what you’re reading now.\n","date":"14 October 2024","permalink":"/posts/blog-ci/","section":"Posts","summary":"CI setup of my personal website using Github Actions","title":"Poor mans CICD for personal website"},{"content":"","date":null,"permalink":"/tags/website/","section":"Tags","summary":"","title":"Website"},{"content":"","date":null,"permalink":"/tags/now/","section":"Tags","summary":"","title":"Now"},{"content":"Here\u0026rsquo;s what I am busy with:\nBuilding out this website - tinkering with Hugo - DONE. Building a image gallery to showcase my woodworking projects - DONE. Thinking about a pull based deploy of this website. I keep committing my changes to Git, and a faithful watcher on my virtual machine that runs the website polls for changes and deploys the changes periodically. Building out a headboard box for a friend. Exploring Github Actions and using it to auto commit. ","date":null,"permalink":"/now/","section":"Aravindh - SRE","summary":"Here\u0026rsquo;s what I am busy with:","title":"Now."},{"content":"Internet obesity #Many websites we routinely visit are just too fat, loading huge amounts of data in the form of javascript files, unoptimized images, web fonts, ads etc. This is just plain inefficient. Take a look of a few sites that I recently visited… Source: My tests using online tool at https://tools.pingdom.com/\nWhy self host? #Why not? It’s fun.\nSay no to walled gardens and own the Internet.\nA new blog #So, when I wanted to build a blog for myself, I decided it must be simple, efficient, fast, modern, and yet look easy on the eyes of the reader. I thought about building a Staic Site Generator(SSG) myself for this purpose, but decided against it considering there are plenty of open source SSGs available already that fits my needs. I chose Hugo. The default theme (Ananke) is already built using my favourite functional CSS framework - Tachyons, So, I’m set from the get go.\nUpdate: I have moved on from Hugo and chose Mkdocs with Material theme instead. Update to an update: I went back to Hugo because the theme Congo fit my needs well, and I had to scratch an itch that mkdocs couldn\u0026rsquo;t..\nMaking it public #Next step on the way is to host this beautiful website somewhere. I’m going to need a domain name, a Virtual Private Server(VPS) to run a webserver, DNS service, and a TLS certificate provider.\nDomain: I purchased a domain through Google domains.\nVPS: A cloud VM on Hetzner.\nDNS :\nGoogle domains provide a free DNS service that probably is more than enough for personal websites. I chose to use a more business oriented DNS service provided by Google for its Google Cloud Platform customers - Google Cloud DNS. Google describes this service as to offer a reliable, resilient, low-latency DNS serving from Google’s worldwide network. Just a better use of the Free tier that Google Cloud Platform offers. I will setup my own DNS server in the future, but let’s use Google’s for now.\nWeb server :\nCaddy is a web server that comes with automatic Let’s Encrypt integration to fetch those TLS certificates(https). It is one of those tools that just works and gets out of the way, highly recommended for personal projects.\nWith a secure webserver and a domain name pointing to it, all I had left to do was SCP the website built locally on my Mac to the web server.\nPerformance #A little tip to optimize images used in any website: Use either the free online service - https://tinypng.com or if you’re using a Mac, try https://imageoptim.com/mac\nThe result of all the above work is a good looking website that happily hits the goal as shown below.\nI hope my website gave you a little inspiration to build/optimize your own. Have fun! and subscribe to my RSS feed if you’d like to see my future posts.\n","date":"15 January 2024","permalink":"/posts/lean_fast_blog/","section":"Posts","summary":"A little inspiration to self-hosting a website without bloat.","title":"Lean and fast self-hosted blog"},{"content":"","date":null,"permalink":"/tags/performance/","section":"Tags","summary":"","title":"Performance"},{"content":"Duties #A sysadmin is responsible for upkeep, configuration, and reliable operation of systems and services. Accidental sysadmin #I was living in Raleigh, North Carolina, working as a Performance Engineer when I decided to move to Denmark to be with my then girlfriend, now wife, and start a family. Moving to another continent entails a lot of change and looking for a job is no different. I learned soon that finding a job as a Performance Engineer in Europe is not as straightforward as it would be in the US. I instead ventured on a job search based on the skills I have. Voila, I found that I could be a starter sysadmin - I have administered systems before, and turns out there is a lot of commonality between \u0026ldquo;understanding the system enough to engineer its performance\u0026rdquo; and \u0026ldquo;administering a system\u0026rdquo;.\nAfter some Skype interviews, and arduous work permit paper work, here I am in Aarhus, Denmark.\nOrientation #Orientation or transition is very important for a sysadmin is very important, particularly so for someone who is in those shoes anew. Here are some observations I made on the first day in the job :\nMy predecessor had left the job a few months before my arrival\nA barn is re-purposed as a capable data center\nThere is no documentation whatsoever about inventory or purpose of the machines\nThe HPC batch scheduler system currently in use (PBS) is causing a lot of pain\nAll of my colleagues are bioinformaticians/researchers except one who had been a sysadmin early in his career\nAll of my colleagues are nice to me and are rooting for me to do a great job\nI have a lot of room to do good and learn\nSo, I buckled up and started the ride.\nReconnaissance #Before I began, I needed to explore and understand what makes up this cluster and what I should be dealing with. Given the lack of documentation, I had to do the reconnaissance on my own. Here is how I went about it..\nnmap - scan the network(s) to determine which hosts are online, what services they offer(web, database, DNS, firewall etc), which operating system versions are they at, ports open in them and more.\nThis resulted in an Excel document that I treated as an inventory from then on.\nNext up, DNS records. Once I found out which servers are providing name resolution, I looked at their records to get a list of all services that could be in use by others. My Excel doc was made slightly better and richer with information.\nNext, searched for a centralised syslog somewhere, and realised there isnt one.\nMoving on, users and groups. Another result of the network probe was the NIS server that was acting as the identity provider. So, logged in there, and grabbed the list of users and groups. With the help of senior colleagues and some common sense poking around, I made notes of who uses the system, and the purpose of the groups. I had never worked with NIS before, and made note of it being a pain point.\nCron jobs : One of the conversations with users of the systems pointed to some cron jobs that happen in the cluster. So, I listed crontab on all machines and obtained information about the cron jobs that were to be cared for.\nAn advantage of a controlled private cluster such as this one is that you can probe the entrypoint and understand which users can get how far. Since the primary interface to the clsuter was via SSH, I probed the sshd_config files to see who has access to go where.\nKnow when to give up #At this point, the facts I learned during reconnaissance made me very uncomfortable to think about upkeeping this setup. There was so much technical debt on this setup that reinvigorating it would be like putting lipstick on a pig.\nPrimary DNS server was an intel Pentium III server that must have been atleast 14 years old.\nSome of the compute nodes are so old that they could not run recent versions of operating system. This holds back the entire cluster from moving on.\nThe job scheduling system keeps crashing because user memory limits could not be enforced.\nNetwork to the storage servers were clearly the bottleneck. Single 1 Gigabit pipe for all compute nodes \u0026lt;-\u0026gt; Storage.\nLack of documentation\nSo, I had the conversation with boss. This cannot go on with \u0026lsquo;maintenance\u0026rsquo;. We need to plan for a new cluster and move to it and leave the burden behind. He agrees.\nWe decide to take a cautious \u0026lsquo;parallel worlds\u0026rsquo; approach. I would keep oiling this old setup until the new setup is built up, tested by a few beta users, and is declared ready for operation.\nTransitions like this are never easy. I needed to do a lot of thinking to make sure I know what I am doing, and that users will have enough information and guidance on their hands when they make the move.\n✨ GHPC ✨ #The hardest problem first - naming the new cluster!\nI settled on GHPC - short for [Global | Genetics | Genomics] High Performance Compute cluster. Pretty smart eh?\nBefore embarking on the architecture of the new GHPC, I decided it needed to incorporate :\nInfrastructure as code - All nodes must be provisioned via PXE boot and simply get to their service level by running an Ansible playbook. Any modifications to that service has to occur in an idempotent fashion via the playbooks.\nSeparation of concerns - Infrastructure services are too important to tolerate influence from other services. They needed to run as independent as possible - Storage server going down should not impact identity servers, DNS servers etc. DNS server going down should not impact access to storage for compute nodes etc.\nAutomation - Anything that can be automated should be automated. An admin\u0026rsquo;s toil should be reduced deciding which automated script to run to fix what.\nKeep it Simple - Prefer simple and effective tools that does one thing well over fancy cool choices that pimp up your CV and cause tech burden.\nHardware - what to buy? #Meticulous selection of hardware and their setup plays a key role in HPC clusters where every bit of performance counts and often makes the difference in how many days/weeks a user\u0026rsquo;s job takes.\nIt is not as easy or straightforward as one might think. The servers with CPUs containing fastest cores or most number of cores, most amount of memory, fastest SSDs doesnt mean they are fit for the work. HPC applications often come with their own unique needs. The role of the person choosing the hardware is to carefully study the needs of the applications that will eventually run on this hardware and how well to satisfy those needs under the given conditions including the monetary limits.\nTo really understand our need, I set out to get a sample of workloads that are often run in the cluster and ran tests with them.\nI have one cluster of workloads that are purely CPU and memory bandwidth bound. Another cluster of simpler workloads that are mostly IO bound and could not care less about the CPU and memory.\nThe above insights made me take a two-pronged approach :\nSpecial purpose hardware optimised for high impact workloads\nCost optimised hardware for general purpose workloads\nI learned quite a lot about systems design, limits, bottlenecks and practiced the art of trading off one for another.\nBelow are just some examples of the motivations behind hardware purchase decisions. There are several other decisions I had to make, but I will keep their rationale for another detailed post.\nIntel Xeon based servers (because we are stuck with the intel compiler for business reasons, and AMD processors doesnt bode well with this situation even if they are arguably better)\nHyperthreading will be turned ON in some servers that service IO bound workloads, while turned OFF in some servers that service workloads that are purely compute bound with little opportunity to make use of HT.\nSome of our workloads are legacy software still bound to a single core. So, we need cpus with faster cores and not many slower cores.\nMost of the jobs make use of local disk to perform their work and copy their output over to central storage upon completion. Using SSDs instead of spinning disks for local storage is a no-brainer.\nAs much as we would love to have the lower latencies offered by the SFP+ transceivers and optical fiber networking, we decided on cheaper 10G Base-T networking as a \u0026ldquo;good enough\u0026rdquo; choice.\nFiguring out what hardware to buy may be fascinating, but the process to procure them, navigating the rules and regulations at a public institution is obviously not. I sought help and was fortunate with senior colleagues and they helped move things through.\nHPC hardware demands a lot more than say building a PC for personal use. Among other things you have to take the following into account as well. Rack space (and how deep they are), power requirements (I needed a 16A outlet while only 10A outlets were nearby), cooling requirements (hot aisle/cold aisle setup, air flow), being social enough and have enough goodwill to get help from colleagues to lift and fit these servers in the racks, and so on.\nGHPC Architecture #I had the golden opportunity to build a cluster from scratch. Doing things right means I will have an easier time maintaining the cluster once it is operational.\nThere is definitely more to architecting a HPC cluster than what I can condense into this long blog post. However, I will give you a brief view of my choices and how they make up GHPC.\nChoices # All compute nodes are identical software wise. They are provisioned via an Ansible playbook. A new compute node can be added in less than 4 minutes.\nNetworking for this cluster is very simple - two \u0026ldquo;top of the rack\u0026rdquo; 10G leaf switches that talk to storage(NAS) via redundant 40G links.\nSimple switched IP network = All nodes can talk see/talk to each other. Good enough for our scale.\nInternal network stays strictly private - all communications in/out to this network from outside flows through \u0026ldquo;Cluster gateway\u0026rdquo; - an OpenBSD server running pf serving as a firewall.\nCentralised identity provider - Replicated FreeIPA servers act as the identity provider. It also provides LDAP auth for internal services like Gitlab, Jupyterlab etc.\nSSH and SFTP are primary interfaces to interact with cluster from outside. SSH access is by key only - NO PASSWORDS. Accessing the SSH server from outside the institution requires second factor authentication(2FA) using Duo.\nInternal services are only available in private networks. Users are expected to SSH tunnel using their identities to access services like gitlab and jupyterlab.\nInfrastructure services like FreeIPA, web servers, slurm controller etc are run virtualised using KVM - makes it easy to snapshot and backup. It also facilitates simply running them on a different host should any physical servers crash.\nBatch job scheduling is taken care of by SLURM - the defacto scheduler in the HPC space.\nWeb servers are intentionally isolated from being able to access the cluster network as a safety measure. Access to web servers are strictly guarded - separate SSH keys other than the ones being used for the cluster. Access only on a need basis using locally stored SSH keys.\nPrimary storage for the cluster are a bunch of custom built ZFS servers (FreeBSD and Linux). Refer to my other posts for how I built them.\nStorage access is provided via simple NFS v3 - it works great, and can even saturate the network pipes upon demand.\nThe primary storage servers are backed up/mirrored using simple automated rsync to a secondary NAS at a different location using dedicated fiber. Note: Simple as it is, it works for us under the assumption that backup will only be used for DR and not business continuity. We expect downtime if the primary storage goes down hard.\nMonitoring - Time series metrics collected from all nodes and services by Prometheus and visualised by Grafana. rsyslog pipes logs to a central syslog server for audit purposes. Filebeats ship logs to a single-node ElasticSearch setup for centralised log analysis.\nAll compute nodes have mitigations turned OFF! - these are private compute nodes that strictly run software from local repos. that extra 10-30% of performance penalty for these mitigations are not worth it here. If untrusted code is run on these machines, I have a bigger problem to deal with than these vulnerabilities.\nRedundant Unbound on OpenBSD works as Authoritative, validating, recursive caching DNS servers.\nThere are other services like database servers (PostgreSQL and IBM DB2) that are left out of the discussion to keep scope manageable.\nPerforming my duties #So, how do I perform the duties of a sysadmin now?\nAfter all, a sysadmin\u0026rsquo;s job is \u0026ldquo;upkeep, configuration and ensure reliable operation\u0026rdquo;.\nUpkeep:\nGoal is to keep GHPC alive/up reasonably. I have my alerts set up before something blows up - example: \u0026lsquo;disk is 80% full\u0026rsquo; and for obvious problems such as \u0026ldquo;node - sky001 is unreachable\u0026rdquo;. These alerts and \u0026ldquo;system service requests\u0026rdquo; take highest priority in my to-do list.\nI spend a reasonable amount of time thinking about why it occurred, collect relevant logs and bring it back to expected service as soon as possible. Some times, that means SSH ing into nodes to figure out and then running an Ansible playbook from controller to fix things, and other times it could be a hard reset on the node and re-provisioning it with Ansible.\nIf the situation is a hardware failure, it simply results in a warranty request, and wait status until the parts arrive.\nConfiguration:\nMost services need configuration to become useful to users. All configuration files and scipts to trigger the configuration exists in Git repositories under sysadmin\u0026rsquo;s thumb. Configuration changes occur only via Ansible Playbooks that change things idempotently so that if I ever were to re-build this service on another hardware, it would simply be an \u0026ldquo;ansible-playbook something.yml\u0026rdquo;.\nReliable operation:\nReliability means the consistency of the system in performing its required functions in its context over time. I respect the need for consistency and believe change for the sake of change is not worth doing - A lot of CI/CD folks would probably balk at me saying this, but that argument is for another blog post.\nI simply refer to the idea that users expect a certain level of dependability on the state of the system. I try best to not do \u0026ldquo;pull the rug\u0026rdquo; changes. Example : A new version of software is made available as xyz-1.02 and xyz-current is sym linked to the latest version. This way, if a user referred to a specific version they will not be impact by my upgrading the tool they use.\nDesign decisions are made with least interruption to current state of affairs. Example: if I bring the central NAS down for maintenance too often, long running jobs would just be restarting again and again wasting precious resources. So, as sysadmin, I do my darned best to keep the system operational and in correct state during normal operation. That does not mean we resist change - simpler configuration changes happen continously, and disruptive changes/upgrades are thoughtfully deferred for our annual planned maintenance where jobs are paused with months\u0026rsquo; notice.\nContinous automation:\nSimple tasks such as permission changes, file transfers, batch job management etc are all automated using Bash or Python scripts. If the state of compute nodes or infrastructure nodes needs changing, then Ansible takes over.\nCompiling software for performance:\nA surprising proportion of my time goes towards maintaining our software repository with tools compiled in way that is best for our hardware. Some scientific software are compiled with Intel compiler as per author\u0026rsquo;s instructions - with double digit performance improvements over standard compiles. Most other tools are simply compiled with -march=native on our hardware.\nPerformance engineering:\nThe amount of difference one can make by simply focussing on these key areas is astounding :\nChoosing the right data structures and algorithms to work with Removing obvious bottlenecks in processing Organising work around resources As a performance engineer, I tend to care a lot more about these and make the best effort to have our jobs, services and hardware run at their optimal efficiency.\nGeneral purpose devices and services often need tailoring for purpose. Simple to say, but takes a whole lot more than I can condense into this subsection. You can look into how I analysed one of our ZFS file servers\u0026rsquo; performance to get an idea about what goes in here.\nPerformance troubleshooting:\nAnother aspect of performance engineering that I get summoned upon is from the user end of it - \u0026ldquo;My job takes 7 days but \u0026lsquo;I need it faster\u0026rsquo; or \u0026lsquo;it took less time last time around\u0026rsquo;\u0026rdquo; etc. This involves performance analysis and often leads down the rabbit hole to find that simple solution.\nFire-fighting:\nYet another duty of a sysadmin is to deal with \u0026ldquo;firefight\u0026quot;s - expected to solve issues before they wreak havoc. You know you got into one when you get emails in the lines of \u0026ldquo;Something happened, and my files are gone. Pls help!\u0026rdquo;.\nThis is the most demanding yet most satisfying aspect of the job. It makes you sweat and also give you war stories to learn from and reminesce at your future job interview. Here are some examples of fires I have put off :\nUser says - \u0026ldquo;I have files that re created in the future in my home directory\u0026rdquo; :- NTP server was down and Chrony failed ot sync.\nSome users can login while others cannot - IPA server not reachable and cached SSSD credentials let some users log in.\nFile server has reduced capacity - Two disks have failed in NAS, and unfortunately in the same RAIDZ2 VDEV. Rush to get hardware replaced and initiate scrub.\nSSH login takes too long for users - NFS servers serving locations in user\u0026rsquo;s $PATH are not reachable.\nOh DNS\u0026hellip;.. the tricky monster that causes problems sometimes rightaway or at times lead to a slow-motion disaster.\nFirewall changes with pf - locking oneself out, or blocking something that we didnt know was being used.\n\u0026ldquo;kinit admin\u0026rdquo; fails with \u0026ldquo;Credentials revoked\u0026rdquo; - Too many failed attempts for user admin on FreeIPA. Get to work with ldapmodify.\nSystemd - oh my! - It is hard to not to cuss in front of colleagues when dealing with a systemd induced madness. This is one anti-unix tool that aspires to \u0026ldquo;Do everything in a needlessly complex and unintuitive way\u0026rdquo;.\nContinous monitoring - \u0026ldquo;on call\u0026rdquo;:\nMonitoring is a key aspect of a sysadmin\u0026rsquo;s job. Keeping an eye on what is going on and catching problems early saves a lot of headache later on. Having the data available helps with root cause analysis and audit purposes. When you are in a firefight, having monitoring systems functional makes a big positive difference. Monitoring done right is an art and science which cannot be condensed into a subsection. It deserves a detailed post for itself.\nI have Prometheus alertmanager set up to catch known common sense issues before they cause trouble. For those other problems that I end up solving manually, the EK(ElasticSearch + Kibana) tool comes in handy to query centralised logs easily and understand what went on and identify the root cause. Such manual interventions often end up adding more alert patterns to my monitoring systems.\nContinously improving documentation:\nAs a sysadmin, I keep two kinds of documentation:\nUser guide - for users of the system, so that users know how, what, where and why around the cluster. Made as clear as possible with loads of screenshots and verbose explanations of services and guidelines. Example - GHPC wiki/userguide\nAdmin handbook - \u0026ldquo;The book\u0026rdquo; expected to be used only by a superuser/admin. This guide has intricate details of the system, commands used to perform actions, notes from issues, post-mortem analysis notes, network diagrams, hardware details, admin workflows and many more. This guide is expected to be passed over to the next sysadmin or whoever takes over in a \u0026ldquo;hit by the bus\u0026rdquo; scenario.\nNecessary skills # Enthusiasm for this line of work.\nProblem solving skills - focus on the big picture, then drill down to details.\nKnow where to look for information to solve a problem.\nAble to automate all repetetive tasks.\nDocument, monitor, log, audit everything.\nAbility to describe technical information in easy-to-understand terms.\nPatience.\nTools # nmap - utility for network discovery and security auditing cronguru - helper utility to write cron tasks mdbook - A static docs site generator written in Rust unbound DNS FreeIPA - Centralised identity solution Ansible - Automation tool rsync - file copying tool with delata transfers Duo - Multi-factor authentication for SSH Slurm - batch job scheduling system Prometheus - Pull-based time series data monitoring system Grafana - Data visualisation tool to chart data in Prometheus and other sources ElasticSearch - search engine to provide full text search on logs Filebeat - Lightweight log shipper for Elastic Search Kibana - visualisation tool for working with logs on ElasticSearch CentOS 8 - popular RHEL based Linux distribution FreeBSD - popular unix based operating system with deep ZFS integration ZFS - A copy-on-write(COW) filesystem Resources #Idempotence is not a medical condition\nPXE boot - How does it work?\nHyperthreading - where each physical CPU core appears as two logical cores to the OS\nGHPC User guide\nConclusion #So, this is how I work as a sysadmin. If you made it this far in a long post, I hope you enjoyed reading it, and learned a thing or two. If you have any corrections, suggestions on this post, contact me. If you\u0026rsquo;d like to work with me or hire me for my next job, please write me a note :-)\n","date":"15 January 2024","permalink":"/posts/role-of-a-sysadmin/","section":"Posts","summary":"From the trenches of High Performance Computing clusters","title":"Role of a sysadmin"},{"content":"","date":null,"permalink":"/tags/sre/","section":"Tags","summary":"","title":"SRE"},{"content":"","date":null,"permalink":"/tags/sysadmin/","section":"Tags","summary":"","title":"Sysadmin"},{"content":"Investments #I invest my savings in the stock market hoping for better returns in the long term than a savings account at a bank. Here is where my money is invested on as of time of writing.\nWhy am I posting this?\nThe below list is what I came up with after scrouging the internet for picks and making guesses. Perhaps it could give you some choices to do your own research and make your bet.\nSymbol Name Currency AIRBUS FR Airbus EUR AAPL Apple inc USD NVDA Nvidia Corp USD LLY Eli Lilly \u0026amp; co USD AMZN Amazon inc USD COST Costco USD META Meta Platforms USD MSFT Microsoft USD TTE Total Energies SE EUR VWRL Vanguard FTSE AW EUR NOVO-B Novo Nordisk A/S DKK SU Schneider Electric SE EUR ABBV AbbVie inc USD VWCE Vanguard FTSE AW EUR GOOG Alphabet Inc USD PLTR Palantir Technologies USD AMD Advanced Micro Devices USD ALO Alstom SA EUR KR Kroger Co USD CAT Caterpillar USD NTNX Nutanix Inc USD BLZE Backblaze Inc USD Refer to this Reddit thread by someone who analysed **** out of SP500 stocks to identify the best ones to bet on.\nand the top ones at the time of writing are.. here\n","date":"12 January 2024","permalink":"/posts/investments-2024/","section":"Posts","summary":"A little inspiration to invest in the stock markets","title":"How and what I invest in 2024"},{"content":"","date":null,"permalink":"/tags/investments/","section":"Tags","summary":"","title":"Investments"},{"content":"","date":null,"permalink":"/categories/life/","section":"Categories","summary":"","title":"Life"},{"content":"A look at write performance with \u0026amp; without compression.\nContext #This is NOT an all-in post about ZFS performance. I built a FreeBSD+ZFS file server recently at work to serve as an offsite backup server. I wanted to run a few synthetic workloads on it and look at how it fares from performance perspective. Mostly for curiosity and learning purposes.\nPerformance #As stated in the notes about building this server, performance was not one of the priorities, as this server will never face our active workload. What I care about from this server is its ability to work with rsync and keep the data synchronised with our primary storage server. With that context, I ran a few write tests to see how good our solution is and what to expect from it in terms of performance.\nWhen it comes to storage performance, there are two important metrics that stand above all. Throughput and Latency. In simple words, throughput, measured in MiB/s is the maximum amount of data the system can transfer in/out per time. Latency or response time, measured in microseconds or milliseconds is the amount of time taken for an io to complete.\nMethodology #Coming from a storage performance engineering background, here is how I approach performance benchmarking.\nThe goal of benchmarking is to simulate as realistically as possible the workflow that the system is going to support, measure the performance of the system and its subsystems, identify bottlenecks, tune subsystems one by one, effectively removing all bottlenecks until desired performance goals are achieved or reaching a state where a bottleneck cannot be removed without physcical change to the system.\nAll that said, I dont intend to spend my time removing all bottlenecks in our ZFS system and make it the fastest ever!. My goal for this post is only on the measure phase of the cycle. Lets get started.\nWhat attributes do I care about?\nSequential write - to see how good the server will handle the data coming in from our primary server through rsync. Sequential read - for when I need to restore files from this server to the primary(less important). I dont care about random read/write, unlink/delete and meta data performance.\nWrite performance with fio #Fio - Flexible I/O tester\nFio is an I/O testing tool that can spawn a number of threads or processes doing a particular type of I/O action as specified by the user, and report I/O performance in many useful ways. Our focus is on throughput and latency.\nOur goal here is to measure sequential write performance. I\u0026rsquo;m going to assume block size to be 128 KiB as ZFS default record size is 128K.\nTest setup: #Install fio as per instructions on its website. Prepare a job file - write_test.fio that we can customise for several tests.\nroot@delorean:/sec_stor/backup/fiotest/fio-master # nano write_test.fio ; seq_write test [global] rw=write kb_base=1024 bs=128k size=2m runtime=180 iodepth=1 directory=/sec_stor/backup/fiotest/ numjobs=1 buffer_compress_percentage=100 refill_buffers buffer_compress_chunk=131072 buffer_pattern=0xdeadbeef end_fsync=true group_reporting [test1] Where,\nkb_base instructs fio to use the binary prefix system instead of the decimal base(Kibibytes(1024 bytes) instead of kilobytes(1000 bytes)).\nbs is blocksize and is set to 128 KiB.\nsize is file size and is set to 2 MiB.\niodepth - we\u0026rsquo;re running on FreeBSD. AFAIK, default ioengine is psync, and iodepth defaults to 1.\ndirectory specifies where test files are created. numjobs is our tunable for unit of work. Example, 10 will instruct Fio to spawn 10 processes each independently working on it\u0026rsquo;s own file of size - file size specified earlier.\nnumjobs is the number of processes that FIO spawns to generate IO. Each of the spawned process will create a file of size specified earlier, and generate IO to that file independent of other processes.\nbuffer_compress_percentage is the knob that controls the compressibility of the generated data.\nrefill_buffers instructs FIO to refill the buffer with random data on every submit instead of re-using the buffer contents.\nbuffer_compress_chunk is simply the size of the compressible pattern. I chose to match it with ZFS record legth which is 128K or 131072 bytes.\nbuffer_pattern is the pattern to use for compressible data. Needs to be specified to prevent FIO default of using zeroes.\nend_fsync instructs FIO to fsync the file contents when a write stage has completed.\ngroup_reporting is to aggregate results of all processes.\nLets run it once to see if it generates 100% compressible data\u0026hellip;\nroot@delorean:/sec_stor/backup/fiotest/fio-master # rm -rf ../test1.* ; sleep 1 ; ./fio write_test.fio test1: (g=0): rw=write, bs=(R) 128KiB-128KiB, (W) 128KiB-128KiB, (T) 128KiB-128KiB, ioengine=psync, iodepth=1 fio-3.8 Starting 1 process test1: Laying out IO file (1 file / 2MiB) test1: (groupid=0, jobs=1): err= 0: pid=86394: Sun Jul 29 00:07:47 2018 write: IOPS=8000, BW=1000MiB/s (1049MB/s)(2048KiB/2msec) clat (usec): min=68, max=122, avg=74.98, stdev=13.10 lat (usec): min=68, max=123, avg=75.20, stdev=13.28 clat percentiles (usec): | 1.00th=[ 69], 5.00th=[ 69], 10.00th=[ 69], 20.00th=[ 71], | 30.00th=[ 71], 40.00th=[ 71], 50.00th=[ 72], 60.00th=[ 72], | 70.00th=[ 73], 80.00th=[ 76], 90.00th=[ 84], 95.00th=[ 123], | 99.00th=[ 123], 99.50th=[ 123], 99.90th=[ 123], 99.95th=[ 123], | 99.99th=[ 123] lat (usec) : 100=93.75%, 250=6.25% cpu : usr=200.00%, sys=0.00%, ctx=0, majf=0, minf=0 IO depths : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, \u0026gt;=64=0.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, \u0026gt;=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, \u0026gt;=64=0.0% issued rwts: total=0,16,0,0 short=0,0,0,0 dropped=0,0,0,0 latency : target=0, window=0, percentile=100.00%, depth=1 Run status group 0 (all jobs): WRITE: bw=1000MiB/s (1049MB/s), 1000MiB/s-1000MiB/s (1049MB/s-1049MB/s), io=2048KiB (2097kB), run=2-2msec Where,\nclat is completion latency, and we can see this test saw min of 68 us and average of 74.98 us.\nbw is throughput and this test achieved 1000MiB/s\nHowever, this was just an example test to manually verify that everything works as intended. Lets view the file it created..\nroot@delorean:/sec_stor/backup/fiotest/fio-master # ls -alh ../test1.0.0 -rw-r--r-- 1 root wheel 2.0M Jul 29 00:07 ../test1.0.0 So, it did create a 2 MiB file as we expected.\nLet\u0026rsquo;s see how much space this file actually uses in the disk\u0026hellip;\nroot@delorean:/sec_stor/backup/fiotest/fio-master # du -sh ../test1.0.0 165K ../test1.0.0 Nice. Our ZFS system compressed this 2 MiB file down to 165 KiB because it was generated with 100% compressibility setting.\nLet\u0026rsquo;s peek into the file to see what content was generated..\nroot@delorean:/sec_stor/backup/fiotest/fio-master # hexdump -C ../test1.0.0 |less 00000000 de ad be ef de ad be ef de ad be ef de ad be ef |................| * 00200000 (END) It\u0026rsquo;s our requested pattern in the entire file because we asked for 100% compression. Let\u0026rsquo;s modify this test slightly to make it generate 0% compressible data..\nModifying the write_test.fio file with following changes,\nbuffer_compress_percentage=100 buffer_pattern=0xdeadbeef to\nbuffer_compress_percentage=0 ;buffer_pattern=0xdeadbeef ; this is commented out. Repeat the run :\nroot@delorean:/sec_stor/backup/fiotest/fio-master # rm -rf ../test1.* ; sleep 1 ; ./fio write_test.fio test1: (g=0): rw=write, bs=(R) 128KiB-128KiB, (W) 128KiB-128KiB, (T) 128KiB-128KiB, ioengine=psync, iodepth=1 fio-3.8 Starting 1 process ~ ~ ~ ~ ~ ~ ~ ~ Timmed for brevity. check actual usage on disk,\nroot@delorean:/sec_stor/backup/fiotest/fio-master # du -sh ../test1.0.0 2.0M ../test1.0.0 It is using all of 2 MiB because it was not compressible as we expected.\nroot@delorean:/sec_stor/backup/fiotest/fio-master # hexdump -C ../test1.0.0 | less 00000000 a8 71 09 48 d3 ad 5f c5 35 2e 2d b0 b5 51 5a 13 |.q.H.._.5.-..QZ.| 00000010 c6 25 eb 1e 20 72 c3 13 b8 64 fa 70 ce 5e 52 18 |.%.. r...d.p.^R.| 00000020 97 4c c3 9b 5c 13 ab 06 92 e9 2c ed 89 14 88 15 |.L..\\.....,.....| 00000030 32 9d dc c8 fa 0b ea 1e a6 93 82 0a 11 dd bd 05 |2...............| 00000040 74 52 7d d7 60 36 39 0a 4e aa b5 71 0d bb 42 1a |tR}.`69.N..q..B.| 00000050 49 b5 0f d9 86 9e 63 12 a9 76 7d 00 49 07 c8 09 |I.....c..v}.I...| ~ ~ ~ ~ ~ ~ ~ ~ Timmed for brevity. As expected, file is filled with random data which was not compressible.\nBased on this setup, I set up a few tests by varying two parameters, compressibility and load(numfiles). Here is how my test matrix looks like..\n0% compressibility 50% compressibility 100% compressibility 1 Proc (1 X 128 GiB) dataset size = 128 GiB TBD TBD TBD 2 Procs (2 X 128 GiB) dataset size = 256 GiB TBD TBD TBD 3 Proc (3 X 128 GiB) dataset size = 384 GiB TBD TBD TBD . . . . . . TBD TBD TBD 9 Proc (9 X 128 GiB) dataset size = 896 GiB TBD TBD TBD Note: data set set size increases in steps of 128 GiB along with the number of processes.\nKeep in mind that my test system has 768 GiB of memory. So I tailored my test in a way that my dataset gets bigger than the total amount of memory at some point during the test.\nWhat we see here is an example of what we call the hockey stick curve in performance engineering \u0026amp; queueing theory. Assuming constant service time, and constant arrival times, the queueing delay and hence response time follows this hockey stick curve. Once throughput hits a ceiling, the response times/latencies shoot up dramatically. The above chart is not as dramatic as the hockey stick because we\u0026rsquo;re looking at averages of both throughput and latencies.\nand finally, fully compressible data..\nWoohoo! if the data is highly compressible, ZFS munches it much faster because there are fewer disk writes. Infact, at peak throughput the disks were averaging only about 5 MiB/s. Looking at the data together,\n0% compressibility 50% compressibility 100% compressibility 1 Proc (1 X 128 GiB) dataset size = 128 GiB 1623 MiB/s @ 45 us 1756 MiB/s @ 50 us 2491 MiB/s @ 44 us 2 Procs (2 X 128 GiB) dataset size = 256 GiB 2597 MiB/s @ 61 us 2623 MiB/s @ 70 us 4606 MiB/s @ 47 us 3 Proc (3 X 128 GiB) dataset size = 384 GiB 2954 MiB/s @ 90 us 2050 MiB/s @ 153 us 5950 MiB/s @ 55 us 4 Proc (4 X 128 GiB) dataset size = 512 GiB 2798 MiB/s @ 139 us 2122 MiB/s @ 202 us 6154 MiB/s @ 68 us 5 Proc (5 X 128 GiB) dataset size = 640 GiB 2528 MiB/s @ 206 us 2388 MiB/s @ 247 us 6548 MiB/s @ 85 us 6 Proc (6 X 128 GiB) dataset size = 768 GiB 2708 MiB/s @ 234 us 2414 MiB/s @ 279 us 6592 MiB/s @ 103 us 7 Proc (7 X 128 GiB) dataset size = 896 GiB 2625 MiB/s @ 292 us 2210 MiB/s @ 364 us 6510 MiB/s @ 122 us 8 Proc (8 X 128 GiB) dataset size = 1024 GiB 2565 MiB/s @ 350 us 2295 MiB/s @ 401 us 6278 MiB/s @ 145 us 9 Proc (9 X 128 GiB) dataset size = 1152 GiB 2674 MiB/s @ 379 us 2332 MiB/s @ 447 us 6348 MiB/s @ 168 us Out of curiosity, I took a look at the performance of the system while the IO test was in progress. Here are some pleasant sights I had.\nOverall disk bandwidth slightly less than 3 GB per second! To put this in perspective, the network interface on this server is a 10Gbps link.\n10Gbps = 1250 MB/s or 1192 MiB/s.\nOur backup server is servicing writes faster than the 10 Gbps network it is connected to! While this is happening, here is htop.. Good that the fio processes are not CPU bound.\nHere is another view of individual disk\u0026rsquo;s util through systat..\nOk, so each of these disks are doing approximately 30 MiB/s. However, the manufacturer rating of these disks are 237 MiB/s..\nDeriving performance out of disks is more complicated than this. The manufacturer ratings dont apply for all conditions in which the IO hits the disks. IO sizes often play a big role in determining max throughput from a disk. Another factor is caching infront of the disks, and in the storage drivers. As an example of such wild swings, here is a snip from Matthew Rocklin\u0026rsquo;s research into disk throughput vs file size.\nThere may be opportunities to remove bottlenecks and further improve performance. But, that would be useless when the 10Gbps network is already a bottleneck.\nConclusion #It was fun looking at the performance of the ZFS server in the context it will be used at. I\u0026rsquo;m amazed particularly by how ZFS handle compressible data with ease. At some point it should become the default. Knowing that the system I built exceeded performance goals is always good. Hopefully, these notes above helps others tailor their test cases to anlyze different scenarios.\n","date":"29 July 2018","permalink":"/posts/zfs-performance/","section":"Posts","summary":"A look at ZFS performance with \u0026amp; without compression.","title":"Analyzing ZFS performance"},{"content":"","date":null,"permalink":"/tags/freebsd/","section":"Tags","summary":"","title":"FreeBSD"},{"content":"","date":null,"permalink":"/tags/nfs/","section":"Tags","summary":"","title":"NFS"},{"content":"","date":null,"permalink":"/tags/zfs/","section":"Tags","summary":"","title":"ZFS"},{"content":"","date":null,"permalink":"/tags/blogroll/","section":"Tags","summary":"","title":"Blogroll"},{"content":"These are the blogs I read.\nJulia Evans Chris Siebenmann - utoronto Brendan Gregg Jason ","date":null,"permalink":"/blogroll/","section":"Aravindh - SRE","summary":"These are the blogs I read.","title":"Blogs I keep my eye on"},{"content":"This is a story of how I built a Storage server based on FreeBSD and ZFS to replace aging NetApp and Isilon storage servers that were serving HPC data over NFS.\nThe need #At work, we run a compute cluster that uses an Isilon cluster as primary NAS storage. Excluding snapshots, we have about 200TB of research data, some of them in compressed formats, and others not. We needed an offsite backup file server that would constantly mirror our primary NAS and serve as a quick recovery source in case of a data loss in the the primary NAS. This offsite file server would be passive - will never face the wrath of the primary cluster workload.\nIn addition to the role of a passive backup server, this solution would take on some passive report generation workloads as an ideal way of offloading some work from the primary NAS. The passive work is read-only.\nThe backup server would keep snapshots in a best effort basis dating back to 10 years. However, this data on this backup server would be archived to tapes periodically.\nSnapshots != Backups.\nA simple guidance of priorities:\nData integrity \u0026gt; Cost of solution \u0026gt; Storage capacity \u0026gt; Performance.\nWhy not enterprise NAS? NetApp FAS or EMC Isilon or the like? #We decided that enterprise grade NAS like NetAPP FAS or EMC Isilon are prohibitively expensive and an overkill for our needs.\nAn opensource \u0026amp; cheaper alternative to enterprise grade filesystem with the level of durability we expect turned up to be ZFS. We\u0026rsquo;re already spoilt from using snapshots by a clever Copy-on-Write Filesystem(WAFL) by NetApp. ZFS providing snapshots in almost identical way was a big influence in the choice. This is also why we did not consider just a CentOS box with the default XFS filesystem.\nFreeBSD vs Debian for ZFS #This is a backup server, a long-term solution. Stability and reliability are key requirements. ZFS on Linux may be popular at this time, but there is a lot of churn around its development, which means there is a higher probability of bugs like this to occur. We\u0026rsquo;re not looking for cutting edge features here. Perhaps, Linux would be considered in the future.\nWe already utilize FreeBSD and OpenBSD for infrastructure services and we have nothing but praises for the stability that the BSDs have provided us. We\u0026rsquo;d gladly use FreeBSD and OpenBSD wherever possible.\nOkay, ZFS, but why not FreeNAS? #IMHO, FreeNAS provides a integrated GUI management tool over FreeBSD for a novice user to setup and configure FreeBSD, ZFS, Jails and many other features. But, this user facing abstraction adds an extra layer of complexity to maintain that is just not worth it in simpler use cases like ours. For someone that appreciates the commandline interface, and understands FreeBSD enough to administer it, plain FreeBSD + ZFS is simpler and more robust than FreeNAS.\nSpecifications # Lenovo SR630 Rackserver 2 X Intel Xeon silver 4110 CPUs 768 GB of DDR4 ECC 2666 MHz RAM 4 port SAS card configured in passthrough mode(JBOD) Intel network card with 10 Gb SFP+ ports 128GB M.2 SSD for use as boot drive 2 X HGST 4U60 JBOD 120(2 X 60) X 10TB SAS disks FreeBSD #Both the JBODs are connected to the rack server with dual SAS cables for connection redundancy. The rack server would see 120 disks attached to it that it can own. The rack server was in turn connected to a switch with a high bandwidth link to the primary storage server. Once the physical setup was complete, it was time to install FreeBSD. Simple vanilla installation of FreeBSD 11.2 based on a USB install media. Nothing out of the ordinary.\nRun updates and install basic tools:\n\u0026gt; freebsd-update fetch \u0026gt; freebsd-update install \u0026gt; pkg upgrade \u0026gt; pkg install nano htop zfsnap screen rsync \u0026gt; echo \u0026#39;sshd_enable=\u0026#34;YES\u0026#34;\u0026#39; \u0026gt;\u0026gt; /etc/rc.conf \u0026gt; service sshd start ZFS #The Z File System, is actually more than just a filesystem. It serves as a volume manager + filesystem. It is almost always better to provide raw disks to ZFS instead of building RAID to make multiple disks appear as one. In our setup we\u0026rsquo;d like for the 120 disks from JBODs to be owned by ZFS.\nEnable ZFS\n\u0026gt; echo \u0026#39;zfs_enable=\u0026#34;YES\u0026#34;\u0026#39; \u0026gt;\u0026gt; /etc/rc.conf \u0026gt; service zfs start Basic ZFS terminology #A storage pool(zpool) is the most basic building block of ZFS. A pool is made up of one or more vdevs, the underlying devices that store the data. A zpool is then used to create one or more file systems (datasets). A vdev is usually a group of disks(RAID).ZFS spreads data across the vdevs to increase performance and maximize usable space.\nWhen building out a ZFS based filesystem, one needs to carefully plan the number and type of vdevs, number of disks in each vdev etc according to their specific needs. Simple fact is that the more vdevs you add, the more ZFS spreads the writes thereby improving performance. However, each vdev(equivalent of a RAID group in NetApp world) dedicates some disk space for parity data to provide the recoverability that we desire from ZFS. This simply translates to another fact that using more number of vdevs will result in reduced usable storage capacity.\nJust to be clear, the parity and redundancy are only within a vdev. if the system loses a disk in a vdev, it holds up, and resilvers a spare disk or awaits a new disk, but still servicing user work. But, if the system loses a vdev, the entire zpool is bust. Keep this in mind when you plan for redundancy.\nGiven that I have 120 disks at my disposal, I needed to choose between the following options on my drawing board. I decided to go with RAIDZ2 so that the system can tolerate simultaneous failure of 2 disks per vdev. Considering that I have hot spares, anything beyond RAIDZ2 would be an overkill for my needs. RAIDZ2 is already beyond our needs.\nRaw storage (plain disks without any RAID) = 10 TB X 120 = 1.2 PB or 1091.4 TiB. As per SI system, Gigabyte(GB) is 1000000000 bytes or 109 bytes. However, per binary prefix system, Gibibyte(GiB) is 1073741824 bytes or 230 bytes. Harddrive manufacturers use GB \u0026amp; TB, but standard unix tools like du and df gives out numbers in KiB, GiB and TiB. So, I\u0026rsquo;ll stick to this convention.\nNumber of vdevs Disks per vdev Spare disks Num parity disks(2 per vdev) Effective storage ratio Usable ZFS storage 9 13 3 18 (120 - 3 spares - 18 parity disks) / 120 = 82.5% 82.5% of 1091.4 TiB = 900.3 TiB 14 8 8 28 (120 - 8 spares - 28 parity disks) / 120 =70% 70% of 1091.4 TiB = 763.7 TiB 11 10 4 22 (110 - 22 parity disks) / 110 =80% 80% of 1000.45 TiB = 800 TiB 1 6 4 2 (6 - 2 parity disks) / 6 =67% 67% of 54.5 TiB = 36.5 TiB 14 NA 4 24 (120 - 4 spares - 24 parity disks) / 120 =76.6% 76.6% of 1091.4 TiB = 836 TiB Each of the three above choice balances a tradeoff between storage capacity, failure tolerance, and performance. I chose to go with the third choice, which is to build two separate pools - one for the data, and other for backing up our infrastructure servers(DNS, FreeIPA, Firewall etc). The data zpool (sec_stor) will have 10 disks per vdev and can tolerate a simultaneous failure of two disks within a vdev(and failure of upto 24 disks if they are distributed as two failures per vdev). The hot spares are expected to kick in the moment even one of them fails, so it is sufficient to keep the data safe.\nashift and a boat load of luck #(updated information after feedback from Reddit r/zfs by Jim Salter - @jrssnet and u/fengshui)\nWhat is ashift? Here is a snip from open-zfs wiki:\nvdevs contain an internal property called ashift, which stands for alignment shift. It is set at vdev creation and it is immutable. It is calculated as the maximum base 2 logarithm of the physical sector size of any child vdev and it alters the disk format such that writes are always done according to it. This makes 2ashift the smallest possible IO on a vdev.\nConfiguring ashift correctly is important because partial sector writes incur a penalty where the sector must be read into a buffer before it can be written. ZFS makes the implicit assumption that the sector size reported by drives is correct and calculates ashift based on that.\nIn an ideal world, ZFS making an automatic choice based on what the disk declares about itself would be sweet. But, the world is not ideal!. You see, Some operating systems, such as Windows XP, were written under the assumption that sector sizes are 512 bytes and will not function when drives report a different sector size. So, instead of not supporting those old operating systems for the newest drives, the disk manufacturers sometimes decide to make the disk lie about its sector size. For example, in Jim\u0026rsquo;s case, a Kingston A400 SSD was advertizing its sector size as 512 bytes, when its actual sector was 8K. The performance cost of this idiocy is anecdotally orders of magnitude high.\nHaving learnt all of this information on Reddit after I deployed the backup server, I franctically tried to find out what ZFS was doing in my case.\nI found the spec sheet for the HGST NAS 10TB SAS drives I used. It claims the disks are \u0026ldquo;Sector Size (Variable, Bytes/sector) 4Kn: 4096 512e: 512\u0026rdquo;. So, at the least these drives support 4K sector sizes.\nFirst from sysctl, about what ZFS is reporting as ashift..\nroot@delorean:/sec_stor/backup/fiotest/fio-master # sysctl -a|grep ashift vfs.zfs.min_auto_ashift: 9 vfs.zfs.max_auto_ashift: 13 This doesnt quite help, as the min_auto_ashift is still 9, which means that it is possible for ZFS to be using 29=512 bytes as sector size. But, it does give a breather that the max was above what I desire - 12.\nSo, I ran zdb to find out what ZFS reports as its running configuration.\nroot@delorean:/sec\\_stor/backup/fiotest/fio-master # zdb | grep ashift ashift: 12 ashift: 12 ashift: 12 ashift: 12 ashift: 12 ashift: 12 ashift: 12 ashift: 12 ashift: 12 ashift: 12 ashift: 12 ashift: 12 Whooof! All 12 of my vdevs are reporting a ashift of 12. Which means that they correctly identified the disks as with 4K sector sizes. I didnt make a mistake with a immutable config parameter purely by luck.\nOkay. on with the original flow of the blog post\u0026hellip;\nMaking ZFS happen #It\u0026rsquo;s time to turn our design choices into actual configuration. As is the norm with FreeBSD, all disks are listed at /dev/da*\nCreate a zpool named sec_stor, and add the first vdev and our 4 hot spares. Then add the rest of the VDEVs.\n\u0026gt; zpool create sec_stor raidz2 /dev/da0 /dev/da1 /dev/da2 /dev/da3 /dev/da4 /dev/da5 /dev/da6 /dev/da7 /dev/da8 /dev/da9 spare /dev/da110 /dev/da111 /dev/da112 /dev/da113 \u0026gt; zpool add sec_stor raidz2 /dev/da10 /dev/da11 /dev/da12 /dev/da13 /dev/da14 /dev/da15 /dev/da16 /dev/da17 /dev/da18 /dev/da19 \u0026gt; zpool add sec_stor raidz2 /dev/da20 /dev/da21 /dev/da22 /dev/da23 /dev/da24 /dev/da25 /dev/da26 /dev/da27 /dev/da28 /dev/da29 \u0026gt; zpool add sec_stor raidz2 /dev/da30 /dev/da31 /dev/da32 /dev/da33 /dev/da34 /dev/da35 /dev/da36 /dev/da37 /dev/da38 /dev/da39 \u0026gt; zpool add sec_stor raidz2 /dev/da40 /dev/da41 /dev/da42 /dev/da43 /dev/da44 /dev/da45 /dev/da46 /dev/da47 /dev/da48 /dev/da49 \u0026gt; zpool add sec_stor raidz2 /dev/da50 /dev/da51 /dev/da52 /dev/da53 /dev/da54 /dev/da55 /dev/da56 /dev/da57 /dev/da58 /dev/da59 \u0026gt; zpool add sec_stor raidz2 /dev/da60 /dev/da61 /dev/da62 /dev/da63 /dev/da64 /dev/da65 /dev/da66 /dev/da67 /dev/da68 /dev/da69 \u0026gt; zpool add sec_stor raidz2 /dev/da70 /dev/da71 /dev/da72 /dev/da73 /dev/da74 /dev/da75 /dev/da76 /dev/da77 /dev/da78 /dev/da79 \u0026gt; zpool add sec_stor raidz2 /dev/da80 /dev/da81 /dev/da82 /dev/da83 /dev/da84 /dev/da85 /dev/da86 /dev/da87 /dev/da88 /dev/da89 \u0026gt; zpool add sec_stor raidz2 /dev/da90 /dev/da91 /dev/da92 /dev/da93 /dev/da94 /dev/da95 /dev/da96 /dev/da97 /dev/da98 /dev/da99 \u0026gt; zpool add sec_stor raidz2 /dev/da100 /dev/da101 /dev/da102 /dev/da103 /dev/da104 /dev/da105 /dev/da106 /dev/da107 /dev/da108 /dev/da109 Verify that the zpool is what we expect it to be and that all devices are online.\nroot@delorean:~ # zpool status pool: sec_stor state: ONLINE scan: none requested config: NAME STATE READ WRITE CKSUM sec_stor ONLINE 0 0 0 raidz2-0 ONLINE 0 0 0 da0 ONLINE 0 0 0 da1 ONLINE 0 0 0 da2 ONLINE 0 0 0 da3 ONLINE 0 0 0 da4 ONLINE 0 0 0 da5 ONLINE 0 0 0 da6 ONLINE 0 0 0 da7 ONLINE 0 0 0 da8 ONLINE 0 0 0 da9 ONLINE 0 0 0 raidz2-1 ONLINE 0 0 0 da10 ONLINE 0 0 0 da11 ONLINE 0 0 0 da12 ONLINE 0 0 0 da13 ONLINE 0 0 0 da14 ONLINE 0 0 0 da15 ONLINE 0 0 0 da16 ONLINE 0 0 0 da17 ONLINE 0 0 0 da18 ONLINE 0 0 0 da19 ONLINE 0 0 0 raidz2-2 ONLINE 0 0 0 da20 ONLINE 0 0 0 da21 ONLINE 0 0 0 da22 ONLINE 0 0 0 da23 ONLINE 0 0 0 da24 ONLINE 0 0 0 da25 ONLINE 0 0 0 da26 ONLINE 0 0 0 da27 ONLINE 0 0 0 da28 ONLINE 0 0 0 da29 ONLINE 0 0 0 ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ trimmed for brevity. raidz2-10 ONLINE 0 0 0 da100 ONLINE 0 0 0 da101 ONLINE 0 0 0 da102 ONLINE 0 0 0 da103 ONLINE 0 0 0 da104 ONLINE 0 0 0 da105 ONLINE 0 0 0 da106 ONLINE 0 0 0 da107 ONLINE 0 0 0 da108 ONLINE 0 0 0 da109 ONLINE 0 0 0 spares da110 AVAIL da111 AVAIL da112 AVAIL da113 AVAIL errors: No known data errors Create the second zpool\nzpool create config_stor raidz2 /dev/da114 /dev/da115 /dev/da116 /dev/da117 /dev/da118 Lets see how much storage we built on this server.\nroot@delorean:~ # df -h Filesystem Size Used Avail Capacity Mounted on /dev/ada0p2 111G 1.8G 100G 2% / devfs 1.0K 1.0K 0B 100% /dev sec_stor 735T 201K 735T 0% /sec_stor config_stor 26T 157K 26T 0% /config_stor The curious case of missing 65TiB ?! #According to the table earlier, I was supposed to have 800 TiB of usable ZFS storage. I see only 735 TiB. Where did the 65 TiB go?\nMy understanding was that the choice RAIDZ2 means that I\u0026rsquo;d lose 2 disks worth of storage space for parity as overhead. But, I was wrong. If you take reservations for parity and padding into account, and add in an extra 2.3% of slop space allocation, it explains the missing 65 TiB of storage capacity. Read through the Reddit post here to see the discussion.\nYou can use the calculator here to make more accurate capacity estimations than I did above.\nFor ZFS to make use of this storage we made available, we need to create a ZFS \u0026ldquo;dataset\u0026rdquo; on top of this. A zfs dataset is synonymous to a filesystem. Before we get to it, we need to think about a couple of ZFS storage efficiency features: Compression and Deduplication.\nStorage efficiency: (Compression and Deduplication) #Compression: (Source: FreeBSD docs) ZFS provides transparent compression. Compressing data at the block level as it is written not only saves space, but can also increase disk throughput. If data is compressed by 25%, but the compressed data is written to the disk at the same rate as the uncompressed version, resulting in an effective write speed of 125%. Compression can also be a great alternative to Deduplication because it does not require additional memory.\nZFS offers several different compression algorithms, each with different trade-offs. The biggest advantage to LZ4 is the early abort feature. If LZ4 does not achieve at least 12.5% compression in the first part of the data, the block is written uncompressed to avoid wasting CPU cycles trying to compress data that is either already compressed or uncompressible.\nDeduplication: When enabled, deduplication uses the checksum of each block to detect duplicate blocks. However, be warned: deduplication requires an extremely large amount of memory, and most of the space savings can be had without the extra cost by enabling compression instead.\nHaving chosen LZ4 compression, we decided that the cost of dedupe in terms of memory requirements is not worth the effort in our usecase.\nZFS datasets #With choice of compression already made, create ZFS datasets using:\nzfs create -o compress=lz4 -o snapdir=visible /sec_stor/backup zfs create -o compress=lz4 -o snapdir=visible config_stor/backup Hot spares #ZFS allows devices to be associated with pools as \u0026ldquo;hot spares\u0026rdquo;. These devices are not actively used in the pool, but when an active device fails, it is automatically replaced by a hot spare. This feature requires a userland helper. FreeBSD provides zfsd(8) for this purpose. It must be manually enabled by adding zfsd_enable=\u0026ldquo;YES\u0026rdquo; to /etc/rc.conf. With choice of compression already made, create ZFS datasets using:\necho \u0026#39;zfsd_enable=“YES”\u0026#39; \u0026gt;\u0026gt; /etc/rc.conf ZFS snapshots (on time) #A key requirement in our solution is snapshots. A snapshot provides a read-only, point-in-time copy of the dataset. In a Copy-On-Write(COW) filesystem such as ZFS, snapshots come with very little cost because they are essentially nothing more than a point of duplication of blocks. Having regular scheduled snapshots, enables an user/administrator to recover deleted files from back in time saving enormous time \u0026amp; effort of restoring files from tape archives. In our specific usecase, it is not very uncommon for a user to request restoration of a research dataset that was intentionally deleted say 6 months ago. Instead of walking over to the archive room, finding the appropriate tapes, loading them into the robot, dealing with NetBackup, and patiently wait while it restores the dataset, I can just do\ncd location/deleted/data cp -R .zfs/snapshot/2018-06-10_13.00.00--30d/* . echo \u0026#34;Sysadmin is happy!\u0026#34; We already installed a package - zfsnap earlier in the setup. zfsnap is a utility that helps with creation and deletion of snapshots. A simple way to create a manual snapshot is possible without installing zfsnap..\nzfs snapshot -r mypool@my_recursive_snapshot But, we installed zfsnap for convenience. As a quick example, here is a one shot command to create a snapshot with a retention period of 10 years:\n/usr/local/sbin/zfSnap -a 10y -r sec_stor/backup As a backup server, I want this system to have a schedule of snapshots so that data can be recovered back in time, with varying levels of granularity. Here is what I came up with in our cron schedule for our needs. snip from /etc/crontab :\n# Run ZFS snapshot daily at 1 PM with retention period of 30 days 0 13 * * * root /usr/local/sbin/zfSnap -a 30d -r sec_stor/backup # Run ZFS snapshot monthly at 2 PM on the first day of the month with retention period of 1 year 0 14 1 * * root /usr/local/sbin/zfSnap -a 1y -r sec_stor/backup # Run ZFS snapshot yearly at 3 PM on 6th January every year with retention period of 10 years 0 15 6 * * root /usr/local/sbin/zfSnap -a 10y -r sec_stor/backup # Run deletion of older stale snapshots at 4 PM on first day of every month 0 16 1 * * root /usr/local/sbin/zfSnap -d sec_stor/backup This way, I get daily snapshots for 30 days, monthly snapshots for a year, yearly snapshots for 10 years. This will be communicated to the users in advance, so that they what to expect in terms of recovery from backups.\nThe last cron job is to clear out older snapshots that expire. For example, a daily snaphot from 35 days ago.\nTo give you an idea, here is the state of the server after a few months in operation:\n$ zfs list -t all NAME USED AVAIL REFER MOUNTPOINT config_stor 1.16M 25.9T 156K /config_stor config_stor/backup 156K 25.9T 156K /config_stor/backup sec_stor 161T 574T 201K /sec_stor sec_stor/backup 161T 574T 140T /sec_stor/backup sec_stor/backup@2018-06-30_13.00.00--30d 6.46G - 133T - sec_stor/backup@2018-07-01_13.00.00--30d 0 - 134T - sec_stor/backup@2018-07-01_14.00.00--1y 0 - 134T - sec_stor/backup@2018-07-01_16.00.00--1m 0 - 134T - sec_stor/backup@2018-07-02_13.00.00--30d 3.52G - 134T - sec_stor/backup@2018-07-03_13.00.00--30d 3.52G - 134T - ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ trimmed for brevity. Another view of the system after a few months of operation, to see how compression is working out for us.\n$ zfs get used,compressratio,compression,logicalused sec_stor/backup NAME PROPERTY VALUE SOURCE sec_stor/backup used 161T - sec_stor/backup compressratio 1.42x - sec_stor/backup compression lz4 local sec_stor/backup logicalused 224T - 1.42x is excellent. Basically, the sytem uses only 161T to store data that is 224T in size. Cool.\nScrub performance #Having disks as big as 10TB, and many of them(even 10) could be a lot of work to scrub/rebuild. I\u0026rsquo;m not directly concerned about the degraded performance during a long scrub/rebuild, but I\u0026rsquo;m concerned that during the long scrub time, the VDEV will be vulnerable (with tolerance of only 1 additional disk failure). I read from a ZFS tuning guide that the following will help in this case, \u0026ldquo;If you\u0026rsquo;re getting horrible performance during a scrub or resilver, the following sysctls can be set:\u0026rdquo;\ncat \u0026lt;\u0026lt;EOF \u0026gt;\u0026gt; /etc/sysctl.conf vfs.zfs.scrub_delay=0 vfs.zfs.top_maxinflight=128 vfs.zfs.resilver_min_time_ms=5000 vfs.zfs.resilver_delay=0 EOF This basically tells ZFS to ignore user side performance and get the scrub done. This will impact your user facing performance, but this being a backup server, we can safely play with these toggles. This change above is pre-mature optimization and is often considered evil to do such a thing. But, since availability is not critical in my use-case, I felt okay doing such a thing ¯\\_(ツ)_/¯\nPerformance #As stated earlier, performance was not a major goal for this setup. However, we\u0026rsquo;ll be missing out on fun if we didnt see some numbers while the system is pushed harder. I tested the write performance of this server with 0%, 50% \u0026amp; 100% compressible data. I detailed my notes on how I went about setting up and viewing results on a follow-up post here.\nEssential zfs commands #For quick reference, here are some zfs related commands I run on the server from time to time to check on the status.\n# List the state of zpool \u0026gt; zpool list # Show status of individual disks in the zpool \u0026gt; zpool status # Show ZFS dataset(filesystem) stats \u0026gt; zfs list # Show percentage savings from compression \u0026gt; zfs get used,compressratio,compression,logicalused \u0026lt;dataset name\u0026gt; # List all available snapshots in the system \u0026gt; zfs list -t filesystem,snapshot # Watch read/write bandwidth in real time for an entire zpool \u0026gt; zpool iostat -Td -v 1 10 # Watch per second R/W bandwidth util on zpool in a terse way \u0026gt; zpool iostat \u0026lt;pool name\u0026gt; 1 # Watch disk util for all disks in real time \u0026gt; systat -iostat -numbers -- 1 # A one stop command to get all ZFS related stats: https://www.freshports.org/sysutils/zfs-stats # Install and run it using: \u0026gt; pkg install zfs-stats \u0026gt; zfs-stats -a # Get configuration information at zpool level \u0026gt; zpool get \u0026lt;pool name\u0026gt; # Get configuration information at zfs level \u0026gt; zfs get all \u0026lt;dataset name\u0026gt; References: #FreeBSD Handbook\n$ Book - FreeBSD Mastery by Michael W. Lucas\nCalomel.org - ZFS health check\nZFS tuning guide\nReddit discussion on zpool choices\nReddit discussion on ZFS configuration\nzfs-stats\nThis blog post on Reddit for feedback\nJim Salter\u0026rsquo;s blog\n","date":"16 July 2018","permalink":"/posts/file-server-freebsd-zfs/","section":"Posts","summary":"This is a story of how I built a Storage server based on FreeBSD and ZFS to replace aging NetApp and Isilon storage servers that were serving HPC data over NFS.","title":"Building a file server with FreeBSD, ZFS and a lot of disks"}]