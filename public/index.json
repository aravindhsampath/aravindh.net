[{"content":" I write about computers, notes on things I find interesting, and my wood working adventures.\n","date":null,"permalink":"/","section":"Aravindh - SRE","summary":"","title":"Aravindh - SRE"},{"content":"","date":null,"permalink":"/tags/now/","section":"Tags","summary":"","title":"Now"},{"content":"Here\u0026rsquo;s what I am busy with:\nBuilding out this website - tinkering with Hugo Building a image gallery to showcase my woodworking projects. Thinking about a pull based deploy of this website. I keep committing my changes to Git, and a faithful watcher on my virtual machine that runs the website polls for changes and deploys the changes periodically. Building out a headboard box for a friend. ","date":null,"permalink":"/now/","section":"Aravindh - SRE","summary":"","title":"Now."},{"content":"","date":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts"},{"content":"","date":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"","date":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"","date":null,"permalink":"/categories/computers/","section":"Categories","summary":"","title":"Computers"},{"content":"","date":null,"permalink":"/tags/hugo/","section":"Tags","summary":"","title":"Hugo"},{"content":"Internet obesity #Many websites we routinely visit are just too fat, loading huge amounts of data in the form of javascript files, unoptimized images, web fonts, ads etc. This is just plain inefficient. Take a look of a few sites that I recently visited… Source: My tests using online tool at https://tools.pingdom.com/\nWhy self host? #Why not? It’s fun.\nSay no to walled gardens and own the Internet.\nA new blog #So, when I wanted to build a blog for myself, I decided it must be simple, efficient, fast, modern, and yet look easy on the eyes of the reader. I thought about building a Staic Site Generator(SSG) myself for this purpose, but decided against it considering there are plenty of open source SSGs available already that fits my needs. I chose Hugo. The default theme (Ananke) is already built using my favourite functional CSS framework - Tachyons, So, I’m set from the get go.\nUpdate: I have moved on from Hugo and chose Mkdocs with Material theme instead. Update to an update: I went back to Hugo because the theme Congo fit my needs well, and I had to scratch an itch that mkdocs couldn\u0026rsquo;t..\nMaking it public #Next step on the way is to host this beautiful website somewhere. I’m going to need a domain name, a Virtual Private Server(VPS) to run a webserver, DNS service, and a TLS certificate provider.\nDomain: I purchased a domain through Google domains.\nVPS: A cloud VM on Hetzner.\nDNS :\nGoogle domains provide a free DNS service that probably is more than enough for personal websites. I chose to use a more business oriented DNS service provided by Google for its Google Cloud Platform customers - Google Cloud DNS. Google describes this service as to offer a reliable, resilient, low-latency DNS serving from Google’s worldwide network. Just a better use of the Free tier that Google Cloud Platform offers. I will setup my own DNS server in the future, but let’s use Google’s for now.\nWeb server :\nCaddy is a web server that comes with automatic Let’s Encrypt integration to fetch those TLS certificates(https). It is one of those tools that just works and gets out of the way, highly recommended for personal projects.\nWith a secure webserver and a domain name pointing to it, all I had left to do was SCP the website built locally on my Mac to the web server.\nPerformance #A little tip to optimize images used in any website: Use either the free online service - https://tinypng.com or if you’re using a Mac, try https://imageoptim.com/mac\nThe result of all the above work is a good looking website that happily hits the goal as shown below.\nI hope my website gave you a little inspiration to build/optimize your own. Have fun! and subscribe to my RSS feed if you’d like to see my future posts.\n","date":"15 January 2024","permalink":"/posts/lean_fast_blog/","section":"Posts","summary":"A little inspiration to self-hosting a website without bloat.","title":"Lean and fast self-hosted blog"},{"content":"","date":null,"permalink":"/tags/performance/","section":"Tags","summary":"","title":"Performance"},{"content":"","date":null,"permalink":"/tags/website/","section":"Tags","summary":"","title":"Website"},{"content":"Investments #I invest my savings in the stock market hoping for better returns in the long term than a savings account at a bank. Here is where my money is invested on as of time of writing.\nWhy am I posting this?\nThe below list is what I came up with after scrouging the internet for picks and making guesses. Perhaps it could give you some choices to do your own research and make your bet.\nSymbol Name Currency AIRBUS FR Airbus EUR AAPL Apple inc USD NVDA Nvidia Corp USD LLY Eli Lilly \u0026amp; co USD AMZN Amazon inc USD COST Costco USD META Meta Platforms USD MSFT Microsoft USD TTE Total Energies SE EUR VWRL Vanguard FTSE AW EUR NOVO-B Novo Nordisk A/S DKK SU Schneider Electric SE EUR ABBV AbbVie inc USD VWCE Vanguard FTSE AW EUR GOOG Alphabet Inc USD PLTR Palantir Technologies USD AMD Advanced Micro Devices USD ALO Alstom SA EUR KR Kroger Co USD CAT Caterpillar USD NTNX Nutanix Inc USD BLZE Backblaze Inc USD Refer to this Reddit thread by someone who analysed **** out of SP500 stocks to identify the best ones to bet on.\nand the top ones at the time of writing are.. here\n","date":"12 January 2024","permalink":"/posts/investments-2024/","section":"Posts","summary":"A little inspiration to invest in the stock markets","title":"How and what I invest in 2024"},{"content":"","date":null,"permalink":"/tags/investments/","section":"Tags","summary":"","title":"Investments"},{"content":"","date":null,"permalink":"/categories/life/","section":"Categories","summary":"","title":"Life"},{"content":"A look at write performance with \u0026amp; without compression.\nContext #This is NOT an all-in post about ZFS performance. I built a FreeBSD+ZFS file server recently at work to serve as an offsite backup server. I wanted to run a few synthetic workloads on it and look at how it fares from performance perspective. Mostly for curiosity and learning purposes.\nPerformance #As stated in the notes about building this server, performance was not one of the priorities, as this server will never face our active workload. What I care about from this server is its ability to work with rsync and keep the data synchronised with our primary storage server. With that context, I ran a few write tests to see how good our solution is and what to expect from it in terms of performance.\nWhen it comes to storage performance, there are two important metrics that stand above all. Throughput and Latency. In simple words, throughput, measured in MiB/s is the maximum amount of data the system can transfer in/out per time. Latency or response time, measured in microseconds or milliseconds is the amount of time taken for an io to complete.\nMethodology #Coming from a storage performance engineering background, here is how I approach performance benchmarking.\nThe goal of benchmarking is to simulate as realistically as possible the workflow that the system is going to support, measure the performance of the system and its subsystems, identify bottlenecks, tune subsystems one by one, effectively removing all bottlenecks until desired performance goals are achieved or reaching a state where a bottleneck cannot be removed without physcical change to the system.\nAll that said, I dont intend to spend my time removing all bottlenecks in our ZFS system and make it the fastest ever!. My goal for this post is only on the measure phase of the cycle. Lets get started.\nWhat attributes do I care about?\nSequential write - to see how good the server will handle the data coming in from our primary server through rsync. Sequential read - for when I need to restore files from this server to the primary(less important). I dont care about random read/write, unlink/delete and meta data performance.\nWrite performance with fio #Fio - Flexible I/O tester\nFio is an I/O testing tool that can spawn a number of threads or processes doing a particular type of I/O action as specified by the user, and report I/O performance in many useful ways. Our focus is on throughput and latency.\nOur goal here is to measure sequential write performance. I\u0026rsquo;m going to assume block size to be 128 KiB as ZFS default record size is 128K.\nTest setup: #Install fio as per instructions on its website. Prepare a job file - write_test.fio that we can customise for several tests.\nroot@delorean:/sec_stor/backup/fiotest/fio-master # nano write_test.fio ; seq_write test [global] rw=write kb_base=1024 bs=128k size=2m runtime=180 iodepth=1 directory=/sec_stor/backup/fiotest/ numjobs=1 buffer_compress_percentage=100 refill_buffers buffer_compress_chunk=131072 buffer_pattern=0xdeadbeef end_fsync=true group_reporting [test1] Where,\nkb_base instructs fio to use the binary prefix system instead of the decimal base(Kibibytes(1024 bytes) instead of kilobytes(1000 bytes)).\nbs is blocksize and is set to 128 KiB.\nsize is file size and is set to 2 MiB.\niodepth - we\u0026rsquo;re running on FreeBSD. AFAIK, default ioengine is psync, and iodepth defaults to 1.\ndirectory specifies where test files are created. numjobs is our tunable for unit of work. Example, 10 will instruct Fio to spawn 10 processes each independently working on it\u0026rsquo;s own file of size - file size specified earlier.\nnumjobs is the number of processes that FIO spawns to generate IO. Each of the spawned process will create a file of size specified earlier, and generate IO to that file independent of other processes.\nbuffer_compress_percentage is the knob that controls the compressibility of the generated data.\nrefill_buffers instructs FIO to refill the buffer with random data on every submit instead of re-using the buffer contents.\nbuffer_compress_chunk is simply the size of the compressible pattern. I chose to match it with ZFS record legth which is 128K or 131072 bytes.\nbuffer_pattern is the pattern to use for compressible data. Needs to be specified to prevent FIO default of using zeroes.\nend_fsync instructs FIO to fsync the file contents when a write stage has completed.\ngroup_reporting is to aggregate results of all processes.\nLets run it once to see if it generates 100% compressible data\u0026hellip;\nroot@delorean:/sec_stor/backup/fiotest/fio-master # rm -rf ../test1.* ; sleep 1 ; ./fio write_test.fio test1: (g=0): rw=write, bs=(R) 128KiB-128KiB, (W) 128KiB-128KiB, (T) 128KiB-128KiB, ioengine=psync, iodepth=1 fio-3.8 Starting 1 process test1: Laying out IO file (1 file / 2MiB) test1: (groupid=0, jobs=1): err= 0: pid=86394: Sun Jul 29 00:07:47 2018 write: IOPS=8000, BW=1000MiB/s (1049MB/s)(2048KiB/2msec) clat (usec): min=68, max=122, avg=74.98, stdev=13.10 lat (usec): min=68, max=123, avg=75.20, stdev=13.28 clat percentiles (usec): | 1.00th=[ 69], 5.00th=[ 69], 10.00th=[ 69], 20.00th=[ 71], | 30.00th=[ 71], 40.00th=[ 71], 50.00th=[ 72], 60.00th=[ 72], | 70.00th=[ 73], 80.00th=[ 76], 90.00th=[ 84], 95.00th=[ 123], | 99.00th=[ 123], 99.50th=[ 123], 99.90th=[ 123], 99.95th=[ 123], | 99.99th=[ 123] lat (usec) : 100=93.75%, 250=6.25% cpu : usr=200.00%, sys=0.00%, ctx=0, majf=0, minf=0 IO depths : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, \u0026gt;=64=0.0% submit : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, \u0026gt;=64=0.0% complete : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, \u0026gt;=64=0.0% issued rwts: total=0,16,0,0 short=0,0,0,0 dropped=0,0,0,0 latency : target=0, window=0, percentile=100.00%, depth=1 Run status group 0 (all jobs): WRITE: bw=1000MiB/s (1049MB/s), 1000MiB/s-1000MiB/s (1049MB/s-1049MB/s), io=2048KiB (2097kB), run=2-2msec Where,\nclat is completion latency, and we can see this test saw min of 68 us and average of 74.98 us.\nbw is throughput and this test achieved 1000MiB/s\nHowever, this was just an example test to manually verify that everything works as intended. Lets view the file it created..\nroot@delorean:/sec_stor/backup/fiotest/fio-master # ls -alh ../test1.0.0 -rw-r--r-- 1 root wheel 2.0M Jul 29 00:07 ../test1.0.0 So, it did create a 2 MiB file as we expected.\nLet\u0026rsquo;s see how much space this file actually uses in the disk\u0026hellip;\nroot@delorean:/sec_stor/backup/fiotest/fio-master # du -sh ../test1.0.0 165K ../test1.0.0 Nice. Our ZFS system compressed this 2 MiB file down to 165 KiB because it was generated with 100% compressibility setting.\nLet\u0026rsquo;s peek into the file to see what content was generated..\nroot@delorean:/sec_stor/backup/fiotest/fio-master # hexdump -C ../test1.0.0 |less 00000000 de ad be ef de ad be ef de ad be ef de ad be ef |................| * 00200000 (END) It\u0026rsquo;s our requested pattern in the entire file because we asked for 100% compression. Let\u0026rsquo;s modify this test slightly to make it generate 0% compressible data..\nModifying the write_test.fio file with following changes,\nbuffer_compress_percentage=100 buffer_pattern=0xdeadbeef to\nbuffer_compress_percentage=0 ;buffer_pattern=0xdeadbeef ; this is commented out. Repeat the run :\nroot@delorean:/sec_stor/backup/fiotest/fio-master # rm -rf ../test1.* ; sleep 1 ; ./fio write_test.fio test1: (g=0): rw=write, bs=(R) 128KiB-128KiB, (W) 128KiB-128KiB, (T) 128KiB-128KiB, ioengine=psync, iodepth=1 fio-3.8 Starting 1 process ~ ~ ~ ~ ~ ~ ~ ~ Timmed for brevity. check actual usage on disk,\nroot@delorean:/sec_stor/backup/fiotest/fio-master # du -sh ../test1.0.0 2.0M ../test1.0.0 It is using all of 2 MiB because it was not compressible as we expected.\nroot@delorean:/sec_stor/backup/fiotest/fio-master # hexdump -C ../test1.0.0 | less 00000000 a8 71 09 48 d3 ad 5f c5 35 2e 2d b0 b5 51 5a 13 |.q.H.._.5.-..QZ.| 00000010 c6 25 eb 1e 20 72 c3 13 b8 64 fa 70 ce 5e 52 18 |.%.. r...d.p.^R.| 00000020 97 4c c3 9b 5c 13 ab 06 92 e9 2c ed 89 14 88 15 |.L..\\.....,.....| 00000030 32 9d dc c8 fa 0b ea 1e a6 93 82 0a 11 dd bd 05 |2...............| 00000040 74 52 7d d7 60 36 39 0a 4e aa b5 71 0d bb 42 1a |tR}.`69.N..q..B.| 00000050 49 b5 0f d9 86 9e 63 12 a9 76 7d 00 49 07 c8 09 |I.....c..v}.I...| ~ ~ ~ ~ ~ ~ ~ ~ Timmed for brevity. As expected, file is filled with random data which was not compressible.\nBased on this setup, I set up a few tests by varying two parameters, compressibility and load(numfiles). Here is how my test matrix looks like..\n0% compressibility 50% compressibility 100% compressibility 1 Proc (1 X 128 GiB) dataset size = 128 GiB TBD TBD TBD 2 Procs (2 X 128 GiB) dataset size = 256 GiB TBD TBD TBD 3 Proc (3 X 128 GiB) dataset size = 384 GiB TBD TBD TBD . . . . . . TBD TBD TBD 9 Proc (9 X 128 GiB) dataset size = 896 GiB TBD TBD TBD Note: data set set size increases in steps of 128 GiB along with the number of processes.\nKeep in mind that my test system has 768 GiB of memory. So I tailored my test in a way that my dataset gets bigger than the total amount of memory at some point during the test.\nWhat we see here is an example of what we call the hockey stick curve in performance engineering \u0026amp; queueing theory. Assuming constant service time, and constant arrival times, the queueing delay and hence response time follows this hockey stick curve. Once throughput hits a ceiling, the response times/latencies shoot up dramatically. The above chart is not as dramatic as the hockey stick because we\u0026rsquo;re looking at averages of both throughput and latencies.\nand finally, fully compressible data..\nWoohoo! if the data is highly compressible, ZFS munches it much faster because there are fewer disk writes. Infact, at peak throughput the disks were averaging only about 5 MiB/s. Looking at the data together,\n0% compressibility 50% compressibility 100% compressibility 1 Proc (1 X 128 GiB) dataset size = 128 GiB 1623 MiB/s @ 45 us 1756 MiB/s @ 50 us 2491 MiB/s @ 44 us 2 Procs (2 X 128 GiB) dataset size = 256 GiB 2597 MiB/s @ 61 us 2623 MiB/s @ 70 us 4606 MiB/s @ 47 us 3 Proc (3 X 128 GiB) dataset size = 384 GiB 2954 MiB/s @ 90 us 2050 MiB/s @ 153 us 5950 MiB/s @ 55 us 4 Proc (4 X 128 GiB) dataset size = 512 GiB 2798 MiB/s @ 139 us 2122 MiB/s @ 202 us 6154 MiB/s @ 68 us 5 Proc (5 X 128 GiB) dataset size = 640 GiB 2528 MiB/s @ 206 us 2388 MiB/s @ 247 us 6548 MiB/s @ 85 us 6 Proc (6 X 128 GiB) dataset size = 768 GiB 2708 MiB/s @ 234 us 2414 MiB/s @ 279 us 6592 MiB/s @ 103 us 7 Proc (7 X 128 GiB) dataset size = 896 GiB 2625 MiB/s @ 292 us 2210 MiB/s @ 364 us 6510 MiB/s @ 122 us 8 Proc (8 X 128 GiB) dataset size = 1024 GiB 2565 MiB/s @ 350 us 2295 MiB/s @ 401 us 6278 MiB/s @ 145 us 9 Proc (9 X 128 GiB) dataset size = 1152 GiB 2674 MiB/s @ 379 us 2332 MiB/s @ 447 us 6348 MiB/s @ 168 us Out of curiosity, I took a look at the performance of the system while the IO test was in progress. Here are some pleasant sights I had.\nOverall disk bandwidth slightly less than 3 GB per second! To put this in perspective, the network interface on this server is a 10Gbps link.\n10Gbps = 1250 MB/s or 1192 MiB/s.\nOur backup server is servicing writes faster than the 10 Gbps network it is connected to! While this is happening, here is htop.. Good that the fio processes are not CPU bound.\nHere is another view of individual disk\u0026rsquo;s util through systat..\nOk, so each of these disks are doing approximately 30 MiB/s. However, the manufacturer rating of these disks are 237 MiB/s..\nDeriving performance out of disks is more complicated than this. The manufacturer ratings dont apply for all conditions in which the IO hits the disks. IO sizes often play a big role in determining max throughput from a disk. Another factor is caching infront of the disks, and in the storage drivers. As an example of such wild swings, here is a snip from Matthew Rocklin\u0026rsquo;s research into disk throughput vs file size.\nThere may be opportunities to remove bottlenecks and further improve performance. But, that would be useless when the 10Gbps network is already a bottleneck.\nConclusion #It was fun looking at the performance of the ZFS server in the context it will be used at. I\u0026rsquo;m amazed particularly by how ZFS handle compressible data with ease. At some point it should become the default. Knowing that the system I built exceeded performance goals is always good. Hopefully, these notes above helps others tailor their test cases to anlyze different scenarios.\n","date":"29 July 2018","permalink":"/posts/zfs-performance/","section":"Posts","summary":"A look at ZFS performance with \u0026amp; without compression.","title":"Analyzing ZFS performance"},{"content":"","date":null,"permalink":"/tags/freebsd/","section":"Tags","summary":"","title":"FreeBSD"},{"content":"","date":null,"permalink":"/tags/nfs/","section":"Tags","summary":"","title":"NFS"},{"content":"","date":null,"permalink":"/tags/sysadmin/","section":"Tags","summary":"","title":"Sysadmin"},{"content":"","date":null,"permalink":"/tags/zfs/","section":"Tags","summary":"","title":"ZFS"},{"content":"","date":null,"permalink":"/tags/blogroll/","section":"Tags","summary":"","title":"Blogroll"},{"content":"These are the blogs I read.\nJulia Evans Chris Siebenmann - utoronto Brendan Gregg ","date":null,"permalink":"/blogroll/","section":"Aravindh - SRE","summary":"","title":"Blogs I keep my eye on"},{"content":"This is a story of how I built a Storage server based on FreeBSD and ZFS to replace aging NetApp and Isilon storage servers that were serving HPC data over NFS.\nThe need #At work, we run a compute cluster that uses an Isilon cluster as primary NAS storage. Excluding snapshots, we have about 200TB of research data, some of them in compressed formats, and others not. We needed an offsite backup file server that would constantly mirror our primary NAS and serve as a quick recovery source in case of a data loss in the the primary NAS. This offsite file server would be passive - will never face the wrath of the primary cluster workload.\nIn addition to the role of a passive backup server, this solution would take on some passive report generation workloads as an ideal way of offloading some work from the primary NAS. The passive work is read-only.\nThe backup server would keep snapshots in a best effort basis dating back to 10 years. However, this data on this backup server would be archived to tapes periodically.\nSnapshots != Backups.\nA simple guidance of priorities:\nData integrity \u0026gt; Cost of solution \u0026gt; Storage capacity \u0026gt; Performance.\nWhy not enterprise NAS? NetApp FAS or EMC Isilon or the like? #We decided that enterprise grade NAS like NetAPP FAS or EMC Isilon are prohibitively expensive and an overkill for our needs.\nAn opensource \u0026amp; cheaper alternative to enterprise grade filesystem with the level of durability we expect turned up to be ZFS. We\u0026rsquo;re already spoilt from using snapshots by a clever Copy-on-Write Filesystem(WAFL) by NetApp. ZFS providing snapshots in almost identical way was a big influence in the choice. This is also why we did not consider just a CentOS box with the default XFS filesystem.\nFreeBSD vs Debian for ZFS #This is a backup server, a long-term solution. Stability and reliability are key requirements. ZFS on Linux may be popular at this time, but there is a lot of churn around its development, which means there is a higher probability of bugs like this to occur. We\u0026rsquo;re not looking for cutting edge features here. Perhaps, Linux would be considered in the future.\nWe already utilize FreeBSD and OpenBSD for infrastructure services and we have nothing but praises for the stability that the BSDs have provided us. We\u0026rsquo;d gladly use FreeBSD and OpenBSD wherever possible.\nOkay, ZFS, but why not FreeNAS? #IMHO, FreeNAS provides a integrated GUI management tool over FreeBSD for a novice user to setup and configure FreeBSD, ZFS, Jails and many other features. But, this user facing abstraction adds an extra layer of complexity to maintain that is just not worth it in simpler use cases like ours. For someone that appreciates the commandline interface, and understands FreeBSD enough to administer it, plain FreeBSD + ZFS is simpler and more robust than FreeNAS.\nSpecifications # Lenovo SR630 Rackserver 2 X Intel Xeon silver 4110 CPUs 768 GB of DDR4 ECC 2666 MHz RAM 4 port SAS card configured in passthrough mode(JBOD) Intel network card with 10 Gb SFP+ ports 128GB M.2 SSD for use as boot drive 2 X HGST 4U60 JBOD 120(2 X 60) X 10TB SAS disks FreeBSD #Both the JBODs are connected to the rack server with dual SAS cables for connection redundancy. The rack server would see 120 disks attached to it that it can own. The rack server was in turn connected to a switch with a high bandwidth link to the primary storage server. Once the physical setup was complete, it was time to install FreeBSD. Simple vanilla installation of FreeBSD 11.2 based on a USB install media. Nothing out of the ordinary.\nRun updates and install basic tools:\n\u0026gt; freebsd-update fetch \u0026gt; freebsd-update install \u0026gt; pkg upgrade \u0026gt; pkg install nano htop zfsnap screen rsync \u0026gt; echo \u0026#39;sshd_enable=\u0026#34;YES\u0026#34;\u0026#39; \u0026gt;\u0026gt; /etc/rc.conf \u0026gt; service sshd start ZFS #The Z File System, is actually more than just a filesystem. It serves as a volume manager + filesystem. It is almost always better to provide raw disks to ZFS instead of building RAID to make multiple disks appear as one. In our setup we\u0026rsquo;d like for the 120 disks from JBODs to be owned by ZFS.\nEnable ZFS\n\u0026gt; echo \u0026#39;zfs_enable=\u0026#34;YES\u0026#34;\u0026#39; \u0026gt;\u0026gt; /etc/rc.conf \u0026gt; service zfs start Basic ZFS terminology #A storage pool(zpool) is the most basic building block of ZFS. A pool is made up of one or more vdevs, the underlying devices that store the data. A zpool is then used to create one or more file systems (datasets). A vdev is usually a group of disks(RAID).ZFS spreads data across the vdevs to increase performance and maximize usable space.\nWhen building out a ZFS based filesystem, one needs to carefully plan the number and type of vdevs, number of disks in each vdev etc according to their specific needs. Simple fact is that the more vdevs you add, the more ZFS spreads the writes thereby improving performance. However, each vdev(equivalent of a RAID group in NetApp world) dedicates some disk space for parity data to provide the recoverability that we desire from ZFS. This simply translates to another fact that using more number of vdevs will result in reduced usable storage capacity.\nJust to be clear, the parity and redundancy are only within a vdev. if the system loses a disk in a vdev, it holds up, and resilvers a spare disk or awaits a new disk, but still servicing user work. But, if the system loses a vdev, the entire zpool is bust. Keep this in mind when you plan for redundancy.\nGiven that I have 120 disks at my disposal, I needed to choose between the following options on my drawing board. I decided to go with RAIDZ2 so that the system can tolerate simultaneous failure of 2 disks per vdev. Considering that I have hot spares, anything beyond RAIDZ2 would be an overkill for my needs. RAIDZ2 is already beyond our needs.\nRaw storage (plain disks without any RAID) = 10 TB X 120 = 1.2 PB or 1091.4 TiB. As per SI system, Gigabyte(GB) is 1000000000 bytes or 109 bytes. However, per binary prefix system, Gibibyte(GiB) is 1073741824 bytes or 230 bytes. Harddrive manufacturers use GB \u0026amp; TB, but standard unix tools like du and df gives out numbers in KiB, GiB and TiB. So, I\u0026rsquo;ll stick to this convention.\nNumber of vdevs Disks per vdev Spare disks Num parity disks(2 per vdev) Effective storage ratio Usable ZFS storage 9 13 3 18 (120 - 3 spares - 18 parity disks) / 120 = 82.5% 82.5% of 1091.4 TiB = 900.3 TiB 14 8 8 28 (120 - 8 spares - 28 parity disks) / 120 =70% 70% of 1091.4 TiB = 763.7 TiB 11 10 4 22 (110 - 22 parity disks) / 110 =80% 80% of 1000.45 TiB = 800 TiB 1 6 4 2 (6 - 2 parity disks) / 6 =67% 67% of 54.5 TiB = 36.5 TiB 14 NA 4 24 (120 - 4 spares - 24 parity disks) / 120 =76.6% 76.6% of 1091.4 TiB = 836 TiB Each of the three above choice balances a tradeoff between storage capacity, failure tolerance, and performance. I chose to go with the third choice, which is to build two separate pools - one for the data, and other for backing up our infrastructure servers(DNS, FreeIPA, Firewall etc). The data zpool (sec_stor) will have 10 disks per vdev and can tolerate a simultaneous failure of two disks within a vdev(and failure of upto 24 disks if they are distributed as two failures per vdev). The hot spares are expected to kick in the moment even one of them fails, so it is sufficient to keep the data safe.\nashift and a boat load of luck #(updated information after feedback from Reddit r/zfs by Jim Salter - @jrssnet and u/fengshui)\nWhat is ashift? Here is a snip from open-zfs wiki:\nvdevs contain an internal property called ashift, which stands for alignment shift. It is set at vdev creation and it is immutable. It is calculated as the maximum base 2 logarithm of the physical sector size of any child vdev and it alters the disk format such that writes are always done according to it. This makes 2ashift the smallest possible IO on a vdev.\nConfiguring ashift correctly is important because partial sector writes incur a penalty where the sector must be read into a buffer before it can be written. ZFS makes the implicit assumption that the sector size reported by drives is correct and calculates ashift based on that.\nIn an ideal world, ZFS making an automatic choice based on what the disk declares about itself would be sweet. But, the world is not ideal!. You see, Some operating systems, such as Windows XP, were written under the assumption that sector sizes are 512 bytes and will not function when drives report a different sector size. So, instead of not supporting those old operating systems for the newest drives, the disk manufacturers sometimes decide to make the disk lie about its sector size. For example, in Jim\u0026rsquo;s case, a Kingston A400 SSD was advertizing its sector size as 512 bytes, when its actual sector was 8K. The performance cost of this idiocy is anecdotally orders of magnitude high.\nHaving learnt all of this information on Reddit after I deployed the backup server, I franctically tried to find out what ZFS was doing in my case.\nI found the spec sheet for the HGST NAS 10TB SAS drives I used. It claims the disks are \u0026ldquo;Sector Size (Variable, Bytes/sector) 4Kn: 4096 512e: 512\u0026rdquo;. So, at the least these drives support 4K sector sizes.\nFirst from sysctl, about what ZFS is reporting as ashift..\nroot@delorean:/sec_stor/backup/fiotest/fio-master # sysctl -a|grep ashift vfs.zfs.min_auto_ashift: 9 vfs.zfs.max_auto_ashift: 13 This doesnt quite help, as the min_auto_ashift is still 9, which means that it is possible for ZFS to be using 29=512 bytes as sector size. But, it does give a breather that the max was above what I desire - 12.\nSo, I ran zdb to find out what ZFS reports as its running configuration.\nroot@delorean:/sec\\_stor/backup/fiotest/fio-master # zdb | grep ashift ashift: 12 ashift: 12 ashift: 12 ashift: 12 ashift: 12 ashift: 12 ashift: 12 ashift: 12 ashift: 12 ashift: 12 ashift: 12 ashift: 12 Whooof! All 12 of my vdevs are reporting a ashift of 12. Which means that they correctly identified the disks as with 4K sector sizes. I didnt make a mistake with a immutable config parameter purely by luck.\nOkay. on with the original flow of the blog post\u0026hellip;\nMaking ZFS happen #It\u0026rsquo;s time to turn our design choices into actual configuration. As is the norm with FreeBSD, all disks are listed at /dev/da*\nCreate a zpool named sec_stor, and add the first vdev and our 4 hot spares. Then add the rest of the VDEVs.\n\u0026gt; zpool create sec_stor raidz2 /dev/da0 /dev/da1 /dev/da2 /dev/da3 /dev/da4 /dev/da5 /dev/da6 /dev/da7 /dev/da8 /dev/da9 spare /dev/da110 /dev/da111 /dev/da112 /dev/da113 \u0026gt; zpool add sec_stor raidz2 /dev/da10 /dev/da11 /dev/da12 /dev/da13 /dev/da14 /dev/da15 /dev/da16 /dev/da17 /dev/da18 /dev/da19 \u0026gt; zpool add sec_stor raidz2 /dev/da20 /dev/da21 /dev/da22 /dev/da23 /dev/da24 /dev/da25 /dev/da26 /dev/da27 /dev/da28 /dev/da29 \u0026gt; zpool add sec_stor raidz2 /dev/da30 /dev/da31 /dev/da32 /dev/da33 /dev/da34 /dev/da35 /dev/da36 /dev/da37 /dev/da38 /dev/da39 \u0026gt; zpool add sec_stor raidz2 /dev/da40 /dev/da41 /dev/da42 /dev/da43 /dev/da44 /dev/da45 /dev/da46 /dev/da47 /dev/da48 /dev/da49 \u0026gt; zpool add sec_stor raidz2 /dev/da50 /dev/da51 /dev/da52 /dev/da53 /dev/da54 /dev/da55 /dev/da56 /dev/da57 /dev/da58 /dev/da59 \u0026gt; zpool add sec_stor raidz2 /dev/da60 /dev/da61 /dev/da62 /dev/da63 /dev/da64 /dev/da65 /dev/da66 /dev/da67 /dev/da68 /dev/da69 \u0026gt; zpool add sec_stor raidz2 /dev/da70 /dev/da71 /dev/da72 /dev/da73 /dev/da74 /dev/da75 /dev/da76 /dev/da77 /dev/da78 /dev/da79 \u0026gt; zpool add sec_stor raidz2 /dev/da80 /dev/da81 /dev/da82 /dev/da83 /dev/da84 /dev/da85 /dev/da86 /dev/da87 /dev/da88 /dev/da89 \u0026gt; zpool add sec_stor raidz2 /dev/da90 /dev/da91 /dev/da92 /dev/da93 /dev/da94 /dev/da95 /dev/da96 /dev/da97 /dev/da98 /dev/da99 \u0026gt; zpool add sec_stor raidz2 /dev/da100 /dev/da101 /dev/da102 /dev/da103 /dev/da104 /dev/da105 /dev/da106 /dev/da107 /dev/da108 /dev/da109 Verify that the zpool is what we expect it to be and that all devices are online.\nroot@delorean:~ # zpool status pool: sec_stor state: ONLINE scan: none requested config: NAME STATE READ WRITE CKSUM sec_stor ONLINE 0 0 0 raidz2-0 ONLINE 0 0 0 da0 ONLINE 0 0 0 da1 ONLINE 0 0 0 da2 ONLINE 0 0 0 da3 ONLINE 0 0 0 da4 ONLINE 0 0 0 da5 ONLINE 0 0 0 da6 ONLINE 0 0 0 da7 ONLINE 0 0 0 da8 ONLINE 0 0 0 da9 ONLINE 0 0 0 raidz2-1 ONLINE 0 0 0 da10 ONLINE 0 0 0 da11 ONLINE 0 0 0 da12 ONLINE 0 0 0 da13 ONLINE 0 0 0 da14 ONLINE 0 0 0 da15 ONLINE 0 0 0 da16 ONLINE 0 0 0 da17 ONLINE 0 0 0 da18 ONLINE 0 0 0 da19 ONLINE 0 0 0 raidz2-2 ONLINE 0 0 0 da20 ONLINE 0 0 0 da21 ONLINE 0 0 0 da22 ONLINE 0 0 0 da23 ONLINE 0 0 0 da24 ONLINE 0 0 0 da25 ONLINE 0 0 0 da26 ONLINE 0 0 0 da27 ONLINE 0 0 0 da28 ONLINE 0 0 0 da29 ONLINE 0 0 0 ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ trimmed for brevity. raidz2-10 ONLINE 0 0 0 da100 ONLINE 0 0 0 da101 ONLINE 0 0 0 da102 ONLINE 0 0 0 da103 ONLINE 0 0 0 da104 ONLINE 0 0 0 da105 ONLINE 0 0 0 da106 ONLINE 0 0 0 da107 ONLINE 0 0 0 da108 ONLINE 0 0 0 da109 ONLINE 0 0 0 spares da110 AVAIL da111 AVAIL da112 AVAIL da113 AVAIL errors: No known data errors Create the second zpool\nzpool create config_stor raidz2 /dev/da114 /dev/da115 /dev/da116 /dev/da117 /dev/da118 Lets see how much storage we built on this server.\nroot@delorean:~ # df -h Filesystem Size Used Avail Capacity Mounted on /dev/ada0p2 111G 1.8G 100G 2% / devfs 1.0K 1.0K 0B 100% /dev sec_stor 735T 201K 735T 0% /sec_stor config_stor 26T 157K 26T 0% /config_stor The curious case of missing 65TiB ?! #According to the table earlier, I was supposed to have 800 TiB of usable ZFS storage. I see only 735 TiB. Where did the 65 TiB go?\nMy understanding was that the choice RAIDZ2 means that I\u0026rsquo;d lose 2 disks worth of storage space for parity as overhead. But, I was wrong. If you take reservations for parity and padding into account, and add in an extra 2.3% of slop space allocation, it explains the missing 65 TiB of storage capacity. Read through the Reddit post here to see the discussion.\nYou can use the calculator here to make more accurate capacity estimations than I did above.\nFor ZFS to make use of this storage we made available, we need to create a ZFS \u0026ldquo;dataset\u0026rdquo; on top of this. A zfs dataset is synonymous to a filesystem. Before we get to it, we need to think about a couple of ZFS storage efficiency features: Compression and Deduplication.\nStorage efficiency: (Compression and Deduplication) #Compression: (Source: FreeBSD docs) ZFS provides transparent compression. Compressing data at the block level as it is written not only saves space, but can also increase disk throughput. If data is compressed by 25%, but the compressed data is written to the disk at the same rate as the uncompressed version, resulting in an effective write speed of 125%. Compression can also be a great alternative to Deduplication because it does not require additional memory.\nZFS offers several different compression algorithms, each with different trade-offs. The biggest advantage to LZ4 is the early abort feature. If LZ4 does not achieve at least 12.5% compression in the first part of the data, the block is written uncompressed to avoid wasting CPU cycles trying to compress data that is either already compressed or uncompressible.\nDeduplication: When enabled, deduplication uses the checksum of each block to detect duplicate blocks. However, be warned: deduplication requires an extremely large amount of memory, and most of the space savings can be had without the extra cost by enabling compression instead.\nHaving chosen LZ4 compression, we decided that the cost of dedupe in terms of memory requirements is not worth the effort in our usecase.\nZFS datasets #With choice of compression already made, create ZFS datasets using:\nzfs create -o compress=lz4 -o snapdir=visible /sec_stor/backup zfs create -o compress=lz4 -o snapdir=visible config_stor/backup Hot spares #ZFS allows devices to be associated with pools as \u0026ldquo;hot spares\u0026rdquo;. These devices are not actively used in the pool, but when an active device fails, it is automatically replaced by a hot spare. This feature requires a userland helper. FreeBSD provides zfsd(8) for this purpose. It must be manually enabled by adding zfsd_enable=\u0026ldquo;YES\u0026rdquo; to /etc/rc.conf. With choice of compression already made, create ZFS datasets using:\necho \u0026#39;zfsd_enable=“YES”\u0026#39; \u0026gt;\u0026gt; /etc/rc.conf ZFS snapshots (on time) #A key requirement in our solution is snapshots. A snapshot provides a read-only, point-in-time copy of the dataset. In a Copy-On-Write(COW) filesystem such as ZFS, snapshots come with very little cost because they are essentially nothing more than a point of duplication of blocks. Having regular scheduled snapshots, enables an user/administrator to recover deleted files from back in time saving enormous time \u0026amp; effort of restoring files from tape archives. In our specific usecase, it is not very uncommon for a user to request restoration of a research dataset that was intentionally deleted say 6 months ago. Instead of walking over to the archive room, finding the appropriate tapes, loading them into the robot, dealing with NetBackup, and patiently wait while it restores the dataset, I can just do\ncd location/deleted/data cp -R .zfs/snapshot/2018-06-10_13.00.00--30d/* . echo \u0026#34;Sysadmin is happy!\u0026#34; We already installed a package - zfsnap earlier in the setup. zfsnap is a utility that helps with creation and deletion of snapshots. A simple way to create a manual snapshot is possible without installing zfsnap..\nzfs snapshot -r mypool@my_recursive_snapshot But, we installed zfsnap for convenience. As a quick example, here is a one shot command to create a snapshot with a retention period of 10 years:\n/usr/local/sbin/zfSnap -a 10y -r sec_stor/backup As a backup server, I want this system to have a schedule of snapshots so that data can be recovered back in time, with varying levels of granularity. Here is what I came up with in our cron schedule for our needs. snip from /etc/crontab :\n# Run ZFS snapshot daily at 1 PM with retention period of 30 days 0 13 * * * root /usr/local/sbin/zfSnap -a 30d -r sec_stor/backup # Run ZFS snapshot monthly at 2 PM on the first day of the month with retention period of 1 year 0 14 1 * * root /usr/local/sbin/zfSnap -a 1y -r sec_stor/backup # Run ZFS snapshot yearly at 3 PM on 6th January every year with retention period of 10 years 0 15 6 * * root /usr/local/sbin/zfSnap -a 10y -r sec_stor/backup # Run deletion of older stale snapshots at 4 PM on first day of every month 0 16 1 * * root /usr/local/sbin/zfSnap -d sec_stor/backup This way, I get daily snapshots for 30 days, monthly snapshots for a year, yearly snapshots for 10 years. This will be communicated to the users in advance, so that they what to expect in terms of recovery from backups.\nThe last cron job is to clear out older snapshots that expire. For example, a daily snaphot from 35 days ago.\nTo give you an idea, here is the state of the server after a few months in operation:\n$ zfs list -t all NAME USED AVAIL REFER MOUNTPOINT config_stor 1.16M 25.9T 156K /config_stor config_stor/backup 156K 25.9T 156K /config_stor/backup sec_stor 161T 574T 201K /sec_stor sec_stor/backup 161T 574T 140T /sec_stor/backup sec_stor/backup@2018-06-30_13.00.00--30d 6.46G - 133T - sec_stor/backup@2018-07-01_13.00.00--30d 0 - 134T - sec_stor/backup@2018-07-01_14.00.00--1y 0 - 134T - sec_stor/backup@2018-07-01_16.00.00--1m 0 - 134T - sec_stor/backup@2018-07-02_13.00.00--30d 3.52G - 134T - sec_stor/backup@2018-07-03_13.00.00--30d 3.52G - 134T - ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ trimmed for brevity. Another view of the system after a few months of operation, to see how compression is working out for us.\n$ zfs get used,compressratio,compression,logicalused sec_stor/backup NAME PROPERTY VALUE SOURCE sec_stor/backup used 161T - sec_stor/backup compressratio 1.42x - sec_stor/backup compression lz4 local sec_stor/backup logicalused 224T - 1.42x is excellent. Basically, the sytem uses only 161T to store data that is 224T in size. Cool.\nScrub performance #Having disks as big as 10TB, and many of them(even 10) could be a lot of work to scrub/rebuild. I\u0026rsquo;m not directly concerned about the degraded performance during a long scrub/rebuild, but I\u0026rsquo;m concerned that during the long scrub time, the VDEV will be vulnerable (with tolerance of only 1 additional disk failure). I read from a ZFS tuning guide that the following will help in this case, \u0026ldquo;If you\u0026rsquo;re getting horrible performance during a scrub or resilver, the following sysctls can be set:\u0026rdquo;\ncat \u0026lt;\u0026lt;EOF \u0026gt;\u0026gt; /etc/sysctl.conf vfs.zfs.scrub_delay=0 vfs.zfs.top_maxinflight=128 vfs.zfs.resilver_min_time_ms=5000 vfs.zfs.resilver_delay=0 EOF This basically tells ZFS to ignore user side performance and get the scrub done. This will impact your user facing performance, but this being a backup server, we can safely play with these toggles. This change above is pre-mature optimization and is often considered evil to do such a thing. But, since availability is not critical in my use-case, I felt okay doing such a thing ¯\\_(ツ)_/¯\nPerformance #As stated earlier, performance was not a major goal for this setup. However, we\u0026rsquo;ll be missing out on fun if we didnt see some numbers while the system is pushed harder. I tested the write performance of this server with 0%, 50% \u0026amp; 100% compressible data. I detailed my notes on how I went about setting up and viewing results on a follow-up post here.\nEssential zfs commands #For quick reference, here are some zfs related commands I run on the server from time to time to check on the status.\n# List the state of zpool \u0026gt; zpool list # Show status of individual disks in the zpool \u0026gt; zpool status # Show ZFS dataset(filesystem) stats \u0026gt; zfs list # Show percentage savings from compression \u0026gt; zfs get used,compressratio,compression,logicalused \u0026lt;dataset name\u0026gt; # List all available snapshots in the system \u0026gt; zfs list -t filesystem,snapshot # Watch read/write bandwidth in real time for an entire zpool \u0026gt; zpool iostat -Td -v 1 10 # Watch per second R/W bandwidth util on zpool in a terse way \u0026gt; zpool iostat \u0026lt;pool name\u0026gt; 1 # Watch disk util for all disks in real time \u0026gt; systat -iostat -numbers -- 1 # A one stop command to get all ZFS related stats: https://www.freshports.org/sysutils/zfs-stats # Install and run it using: \u0026gt; pkg install zfs-stats \u0026gt; zfs-stats -a # Get configuration information at zpool level \u0026gt; zpool get \u0026lt;pool name\u0026gt; # Get configuration information at zfs level \u0026gt; zfs get all \u0026lt;dataset name\u0026gt; References: #FreeBSD Handbook\n$ Book - FreeBSD Mastery by Michael W. Lucas\nCalomel.org - ZFS health check\nZFS tuning guide\nReddit discussion on zpool choices\nReddit discussion on ZFS configuration\nzfs-stats\nThis blog post on Reddit for feedback\nJim Salter\u0026rsquo;s blog\n","date":"16 July 2018","permalink":"/posts/file-server-freebsd-zfs/","section":"Posts","summary":"This is a story of how I built a Storage server based on FreeBSD and ZFS to replace aging NetApp and Isilon storage servers that were serving HPC data over NFS.","title":"Building a file server with FreeBSD, ZFS and a lot of disks"}]