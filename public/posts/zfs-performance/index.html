<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=theme-color content="#FFFFFF"><title>Analyzing ZFS performance &#183; Aravindh.net</title>
<meta name=title content="Analyzing ZFS performance &#183; Aravindh.net"><script type=text/javascript src=/js/appearance.min.8a082f81b27f3cb2ee528df0b0bdc39787034cf2cc34d4669fbc9977c929023c.js integrity="sha256-iggvgbJ/PLLuUo3wsL3Dl4cDTPLMNNRmn7yZd8kpAjw="></script><link type=text/css rel=stylesheet href=/css/main.bundle.min.1be88fb49125ca39fb5a2b42e763687d3a37da0110a95f2f204a959d1bedff8c.css integrity="sha256-G+iPtJElyjn7WitC52NofTo32gEQqV8vIEqVnRvt/4w="><script defer type=text/javascript id=script-bundle src=/js/main.bundle.min.f29ffdffd9ab4cc95250c3c7196b2d5dae8ee6ef0a4139451073f90183ae7e31.js integrity="sha256-8p/9/9mrTMlSUMPHGWstXa6O5u8KQTlFEHP5AYOufjE=" data-copy=Copy data-copied=Copied></script><meta name=description content="
      
        A look at ZFS performance with & without compression.
      
    "><link rel=canonical href=https://aravindh.net/posts/zfs-performance/><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><meta property="og:title" content="Analyzing ZFS performance"><meta property="og:description" content="A look at ZFS performance with & without compression."><meta property="og:type" content="article"><meta property="og:url" content="https://aravindh.net/posts/zfs-performance/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2018-07-29T12:00:00+00:00"><meta property="article:modified_time" content="2018-07-29T12:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Analyzing ZFS performance"><meta name=twitter:description content="A look at ZFS performance with & without compression."><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","articleSection":"Posts","name":"Analyzing ZFS performance","headline":"Analyzing ZFS performance","abstract":"A look at ZFS performance with \u0026amp; without compression.","inLanguage":"en","url":"https:\/\/aravindh.net\/posts\/zfs-performance\/","author":{"@type":"Person","name":""},"copyrightYear":"2018","dateCreated":"2018-07-29T12:00:00\u002b00:00","datePublished":"2018-07-29T12:00:00\u002b00:00","dateModified":"2018-07-29T12:00:00\u002b00:00","keywords":["ZFS","NFS","FreeBSD","Sysadmin","Performance"],"mainEntityOfPage":"true","wordCount":"2085"}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","item":"https://aravindh.net/","name":"Aravindh Sre","position":1},{"@type":"ListItem","item":"https://aravindh.net/posts/","name":"Posts","position":2},{"@type":"ListItem","item":"https://aravindh.net/categories/computers/","name":"Computers","position":3},{"@type":"ListItem","name":"Analyzing Zfs Performance","position":4}]}</script></head><body class="m-auto flex h-screen max-w-7xl flex-col bg-neutral px-6 text-lg leading-7 text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32"><div id=the-top class="absolute flex self-center"><a class="-translate-y-8 rounded-b-lg bg-primary-200 px-3 py-1 text-sm focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="pe-2 font-bold text-primary-600 dark:text-primary-400">&darr;</span>Skip to main content</a></div><header class="py-6 font-semibold text-neutral-900 dark:text-neutral sm:py-10 print:hidden"><nav class="flex items-start justify-between sm:items-center"><div class="flex flex-row items-center"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2" rel=me href=/>Aravindh.net</a></div><ul class="flex list-none flex-col text-end sm:flex-row"><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><button id=appearance-switcher-1 type=button aria-label="appearance switcher">
<span class="group-dark:hover:text-primary-400 inline transition-colors group-hover:text-primary-600 dark:hidden" title="Switch to dark appearance"><span class="icon relative inline-block px-1 align-text-bottom"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg>
</span><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"></span>
</span><span class="group-dark:hover:text-primary-400 hidden transition-colors group-hover:text-primary-600 dark:inline" title="Switch to light appearance"><span class="icon relative inline-block px-1 align-text-bottom"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg>
</span><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"></span></span></button></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><a href=/posts/ title><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">Blog</span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><a href=/wood.html title><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">Woodworking</span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><a href=/tags/ title><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">Tags</span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><a href=/now/ title><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">Now</span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><a href=/blogroll/ title><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2">Blogroll</span></a></li><li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5"><button id=search-button-1 title="Search (/)">
<span class="group-dark:hover:text-primary-400 transition-colors group-hover:text-primary-600"><span class="icon relative inline-block px-1 align-text-bottom"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></span><span class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"></span></button></li></ul></nav></header><div class="relative flex grow flex-col"><main id=main-content class=grow><article><header class=max-w-prose><ol class="text-sm text-neutral-500 dark:text-neutral-400 print:hidden"><li class="hidden inline"><a class="dark:underline-neutral-600 decoration-neutral-300 hover:underline" href=/>Aravindh - SRE</a><span class="px-1 text-primary-500">/</span></li><li class=inline><a class="dark:underline-neutral-600 decoration-neutral-300 hover:underline" href=/posts/>Posts</a><span class="px-1 text-primary-500">/</span></li><li class="hidden inline"><a class="dark:underline-neutral-600 decoration-neutral-300 hover:underline" href=/posts/zfs-performance/>Analyzing ZFS performance</a><span class="px-1 text-primary-500">/</span></li></ol><h1 class="mb-8 mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">Analyzing ZFS performance</h1><div class="mb-10 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime="2018-07-29 12:00:00 +0000 UTC">29 July 2018</time><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">10 mins</span></div></div></header><section class="prose mt-0 flex max-w-full flex-col dark:prose-invert lg:flex-row"><div class="order-first px-0 lg:order-last lg:max-w-xs lg:ps-8"><div class="toc pe-5 lg:sticky lg:top-10 print:hidden"><details open class="-ms-5 mt-0 overflow-hidden rounded-lg ps-5"><summary class="block cursor-pointer bg-neutral-100 py-1 ps-5 text-lg font-semibold text-neutral-800 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="border-s border-dotted border-neutral-300 py-2 ps-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><ul><li><a href=#context>Context</a></li><li><a href=#performance>Performance</a></li><li><a href=#methodology>Methodology</a></li><li><a href=#write-performance-with-fio>Write performance with fio</a><ul><li><a href=#test-setup>Test setup:</a></li></ul></li><li><a href=#conclusion>Conclusion</a></li></ul></li></ul></nav></div></details></div></div><div class="min-h-0 min-w-0 max-w-prose grow"><p>A look at write performance with & without compression.</p><h3 id=context class="relative group">Context <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#context aria-label=Anchor>#</a></span></h3><p>This is NOT an all-in post about ZFS performance. I built <a href=/blog/2018/07/16/building-a-file-server-with-freebsd-zfs-and-a-lot-of-disks/>a FreeBSD+ZFS file server</a> recently at work to serve as an offsite backup server. I wanted to run a few synthetic workloads on it and look at how it fares from performance perspective. Mostly for curiosity and learning purposes.</p><h3 id=performance class="relative group">Performance <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#performance aria-label=Anchor>#</a></span></h3><p>As stated in the notes about building this server, performance was not one of the priorities, as this server will never face our active workload. What I care about from this server is its ability to work with rsync and keep the data synchronised with our primary storage server. With that context, I ran a few write tests to see how good our solution is and what to expect from it in terms of performance.</p><p>When it comes to storage performance, there are two important metrics that stand above all. Throughput and Latency. In simple words, throughput, measured in MiB/s is the maximum amount of data the system can transfer in/out per time. Latency or response time, measured in microseconds or milliseconds is the amount of time taken for an io to complete.</p><h3 id=methodology class="relative group">Methodology <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#methodology aria-label=Anchor>#</a></span></h3><p>Coming from a storage performance engineering background, here is how I approach performance benchmarking.</p><p>The goal of benchmarking is to simulate as realistically as possible the workflow that the system is going to support, measure the performance of the system and its subsystems, identify bottlenecks, tune subsystems one by one, effectively removing all bottlenecks until desired performance goals are achieved or reaching a state where a bottleneck cannot be removed without physcical change to the system.</p><p>All that said, I dont intend to spend my time removing all bottlenecks in our ZFS system and make it the fastest ever!. My goal for this post is only on the <strong>measure</strong> phase of the cycle. Lets get started.</p><p>What attributes do I care about?</p><ol><li>Sequential write - to see how good the server will handle the data coming in from our primary server through rsync.</li><li>Sequential read - for when I need to restore files from this server to the primary(less important).</li></ol><p>I dont care about random read/write, unlink/delete and meta data performance.</p><h3 id=write-performance-with-fio class="relative group">Write performance with fio <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#write-performance-with-fio aria-label=Anchor>#</a></span></h3><p><a href=http://fio.readthedocs.io/en/latest/fio_doc.html target=_blank rel=noreferrer>Fio - Flexible I/O tester</a></p><p><figure><img src=/img/fio.png alt="Flexible IO tester" class="mx-auto my-0 rounded-md"></figure></p><p>Fio is an I/O testing tool that can spawn a number of threads or processes doing a particular type of I/O action as specified by the user, and report I/O performance in many useful ways. Our focus is on throughput and latency.</p><p>Our goal here is to measure sequential write performance. I&rsquo;m going to assume block size to be 128 KiB as ZFS default record size is 128K.</p><h4 id=test-setup class="relative group">Test setup: <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#test-setup aria-label=Anchor>#</a></span></h4><p>Install fio as per instructions on its website. Prepare a job file - write_test.fio that we can customise for several tests.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@delorean:/sec_stor/backup/fiotest/fio-master <span class=c1># nano write_test.fio</span>
</span></span><span class=line><span class=cl><span class=p>;</span> seq_write <span class=nb>test</span>
</span></span><span class=line><span class=cl><span class=o>[</span>global<span class=o>]</span>
</span></span><span class=line><span class=cl><span class=nv>rw</span><span class=o>=</span>write
</span></span><span class=line><span class=cl><span class=nv>kb_base</span><span class=o>=</span><span class=m>1024</span>
</span></span><span class=line><span class=cl><span class=nv>bs</span><span class=o>=</span>128k
</span></span><span class=line><span class=cl><span class=nv>size</span><span class=o>=</span>2m
</span></span><span class=line><span class=cl><span class=nv>runtime</span><span class=o>=</span><span class=m>180</span>
</span></span><span class=line><span class=cl><span class=nv>iodepth</span><span class=o>=</span><span class=m>1</span>
</span></span><span class=line><span class=cl><span class=nv>directory</span><span class=o>=</span>/sec_stor/backup/fiotest/
</span></span><span class=line><span class=cl><span class=nv>numjobs</span><span class=o>=</span><span class=m>1</span>
</span></span><span class=line><span class=cl><span class=nv>buffer_compress_percentage</span><span class=o>=</span><span class=m>100</span>
</span></span><span class=line><span class=cl>refill_buffers
</span></span><span class=line><span class=cl><span class=nv>buffer_compress_chunk</span><span class=o>=</span><span class=m>131072</span>
</span></span><span class=line><span class=cl><span class=nv>buffer_pattern</span><span class=o>=</span>0xdeadbeef
</span></span><span class=line><span class=cl><span class=nv>end_fsync</span><span class=o>=</span><span class=nb>true</span>
</span></span><span class=line><span class=cl>group_reporting
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=o>[</span>test1<span class=o>]</span>
</span></span></code></pre></div><p>Where,</p><p><code>kb_base</code> instructs fio to use the binary prefix system instead of the decimal base(Kibibytes(1024 bytes) instead of kilobytes(1000 bytes)).</p><p><code>bs</code> is blocksize and is set to 128 KiB.</p><p><code>size</code> is file size and is set to 2 MiB.</p><p><code>iodepth</code> - we&rsquo;re running on FreeBSD. AFAIK, default ioengine is psync, and iodepth defaults to <code>1</code>.</p><p><code>directory</code> specifies where test files are created.
numjobs is our tunable for unit of work. Example, <code>10</code> will instruct Fio to spawn <code>10</code> processes each independently working on it&rsquo;s own file of size - file size specified earlier.</p><p><code>numjobs</code> is the number of processes that FIO spawns to generate IO. Each of the spawned process will create a file of size specified earlier, and generate IO to that file independent of other processes.</p><p><code>buffer_compress_percentage</code> is the knob that controls the compressibility of the generated data.</p><p><code>refill_buffers</code> instructs FIO to refill the buffer with random data on every submit instead of re-using the buffer contents.</p><p><code>buffer_compress_chunk</code> is simply the size of the compressible pattern. I chose to match it with ZFS record legth which is 128K or 131072 bytes.</p><p><code>buffer_pattern</code> is the pattern to use for compressible data. Needs to be specified to prevent FIO default of using zeroes.</p><p><code>end_fsync</code> instructs FIO to fsync the file contents when a write stage has completed.</p><p><code>group_reporting</code> is to aggregate results of all processes.</p><p>Lets run it once to see if it generates 100% compressible data&mldr;</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@delorean:/sec_stor/backup/fiotest/fio-master <span class=c1># rm -rf ../test1.* ; sleep 1 ; ./fio write_test.fio</span>
</span></span><span class=line><span class=cl>test1: <span class=o>(</span><span class=nv>g</span><span class=o>=</span>0<span class=o>)</span>: <span class=nv>rw</span><span class=o>=</span>write, <span class=nv>bs</span><span class=o>=(</span>R<span class=o>)</span> 128KiB-128KiB, <span class=o>(</span>W<span class=o>)</span> 128KiB-128KiB, <span class=o>(</span>T<span class=o>)</span> 128KiB-128KiB, <span class=nv>ioengine</span><span class=o>=</span>psync, <span class=nv>iodepth</span><span class=o>=</span><span class=m>1</span>
</span></span><span class=line><span class=cl>fio-3.8
</span></span><span class=line><span class=cl>Starting <span class=m>1</span> process
</span></span><span class=line><span class=cl>test1: Laying out IO file <span class=o>(</span><span class=m>1</span> file / 2MiB<span class=o>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>test1: <span class=o>(</span><span class=nv>groupid</span><span class=o>=</span>0, <span class=nv>jobs</span><span class=o>=</span>1<span class=o>)</span>: <span class=nv>err</span><span class=o>=</span> 0: <span class=nv>pid</span><span class=o>=</span>86394: Sun Jul <span class=m>29</span> 00:07:47 <span class=m>2018</span>
</span></span><span class=line><span class=cl>  write: <span class=nv>IOPS</span><span class=o>=</span>8000, <span class=nv>BW</span><span class=o>=</span>1000MiB/s <span class=o>(</span>1049MB/s<span class=o>)(</span>2048KiB/2msec<span class=o>)</span>
</span></span><span class=line><span class=cl>    clat <span class=o>(</span>usec<span class=o>)</span>: <span class=nv>min</span><span class=o>=</span>68, <span class=nv>max</span><span class=o>=</span>122, <span class=nv>avg</span><span class=o>=</span>74.98, <span class=nv>stdev</span><span class=o>=</span>13.10
</span></span><span class=line><span class=cl>     lat <span class=o>(</span>usec<span class=o>)</span>: <span class=nv>min</span><span class=o>=</span>68, <span class=nv>max</span><span class=o>=</span>123, <span class=nv>avg</span><span class=o>=</span>75.20, <span class=nv>stdev</span><span class=o>=</span>13.28
</span></span><span class=line><span class=cl>    clat percentiles <span class=o>(</span>usec<span class=o>)</span>:
</span></span><span class=line><span class=cl>     <span class=p>|</span>  1.00th<span class=o>=[</span>   69<span class=o>]</span>,  5.00th<span class=o>=[</span>   69<span class=o>]</span>, 10.00th<span class=o>=[</span>   69<span class=o>]</span>, 20.00th<span class=o>=[</span>   71<span class=o>]</span>,
</span></span><span class=line><span class=cl>     <span class=p>|</span> 30.00th<span class=o>=[</span>   71<span class=o>]</span>, 40.00th<span class=o>=[</span>   71<span class=o>]</span>, 50.00th<span class=o>=[</span>   72<span class=o>]</span>, 60.00th<span class=o>=[</span>   72<span class=o>]</span>,
</span></span><span class=line><span class=cl>     <span class=p>|</span> 70.00th<span class=o>=[</span>   73<span class=o>]</span>, 80.00th<span class=o>=[</span>   76<span class=o>]</span>, 90.00th<span class=o>=[</span>   84<span class=o>]</span>, 95.00th<span class=o>=[</span>  123<span class=o>]</span>,
</span></span><span class=line><span class=cl>     <span class=p>|</span> 99.00th<span class=o>=[</span>  123<span class=o>]</span>, 99.50th<span class=o>=[</span>  123<span class=o>]</span>, 99.90th<span class=o>=[</span>  123<span class=o>]</span>, 99.95th<span class=o>=[</span>  123<span class=o>]</span>,
</span></span><span class=line><span class=cl>     <span class=p>|</span> 99.99th<span class=o>=[</span>  123<span class=o>]</span>
</span></span><span class=line><span class=cl>  lat <span class=o>(</span>usec<span class=o>)</span>   : <span class=nv>100</span><span class=o>=</span>93.75%, <span class=nv>250</span><span class=o>=</span>6.25%
</span></span><span class=line><span class=cl>  cpu          : <span class=nv>usr</span><span class=o>=</span>200.00%, <span class=nv>sys</span><span class=o>=</span>0.00%, <span class=nv>ctx</span><span class=o>=</span>0, <span class=nv>majf</span><span class=o>=</span>0, <span class=nv>minf</span><span class=o>=</span><span class=m>0</span>
</span></span><span class=line><span class=cl>  IO depths    : <span class=nv>1</span><span class=o>=</span>100.0%, <span class=nv>2</span><span class=o>=</span>0.0%, <span class=nv>4</span><span class=o>=</span>0.0%, <span class=nv>8</span><span class=o>=</span>0.0%, <span class=nv>16</span><span class=o>=</span>0.0%, <span class=nv>32</span><span class=o>=</span>0.0%, &gt;<span class=o>=</span><span class=nv>64</span><span class=o>=</span>0.0%
</span></span><span class=line><span class=cl>     submit    : <span class=nv>0</span><span class=o>=</span>0.0%, <span class=nv>4</span><span class=o>=</span>100.0%, <span class=nv>8</span><span class=o>=</span>0.0%, <span class=nv>16</span><span class=o>=</span>0.0%, <span class=nv>32</span><span class=o>=</span>0.0%, <span class=nv>64</span><span class=o>=</span>0.0%, &gt;<span class=o>=</span><span class=nv>64</span><span class=o>=</span>0.0%
</span></span><span class=line><span class=cl>     <span class=nb>complete</span>  : <span class=nv>0</span><span class=o>=</span>0.0%, <span class=nv>4</span><span class=o>=</span>100.0%, <span class=nv>8</span><span class=o>=</span>0.0%, <span class=nv>16</span><span class=o>=</span>0.0%, <span class=nv>32</span><span class=o>=</span>0.0%, <span class=nv>64</span><span class=o>=</span>0.0%, &gt;<span class=o>=</span><span class=nv>64</span><span class=o>=</span>0.0%
</span></span><span class=line><span class=cl>     issued rwts: <span class=nv>total</span><span class=o>=</span>0,16,0,0 <span class=nv>short</span><span class=o>=</span>0,0,0,0 <span class=nv>dropped</span><span class=o>=</span>0,0,0,0
</span></span><span class=line><span class=cl>     latency   : <span class=nv>target</span><span class=o>=</span>0, <span class=nv>window</span><span class=o>=</span>0, <span class=nv>percentile</span><span class=o>=</span>100.00%, <span class=nv>depth</span><span class=o>=</span><span class=m>1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>Run status group <span class=m>0</span> <span class=o>(</span>all <span class=nb>jobs</span><span class=o>)</span>:
</span></span><span class=line><span class=cl>  WRITE: <span class=nv>bw</span><span class=o>=</span>1000MiB/s <span class=o>(</span>1049MB/s<span class=o>)</span>, 1000MiB/s-1000MiB/s <span class=o>(</span>1049MB/s-1049MB/s<span class=o>)</span>, <span class=nv>io</span><span class=o>=</span>2048KiB <span class=o>(</span>2097kB<span class=o>)</span>, <span class=nv>run</span><span class=o>=</span>2-2msec
</span></span></code></pre></div><p>Where,</p><p><code>clat</code> is completion latency, and we can see this test saw min of <code>68 us</code> and average of <code>74.98 us</code>.</p><p><code>bw</code> is throughput and this test achieved <code>1000MiB/s</code></p><p>However, this was just an example test to manually verify that everything works as intended. Lets view the file it created..</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@delorean:/sec_stor/backup/fiotest/fio-master <span class=c1># ls -alh ../test1.0.0 </span>
</span></span><span class=line><span class=cl>-rw-r--r--  <span class=m>1</span> root  wheel   2.0M Jul <span class=m>29</span> 00:07 ../test1.0.0
</span></span></code></pre></div><p>So, it did create a 2 MiB file as we expected.</p><p>Let&rsquo;s see how much space this file actually uses in the disk&mldr;</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@delorean:/sec_stor/backup/fiotest/fio-master <span class=c1># du -sh ../test1.0.0</span>
</span></span><span class=line><span class=cl>165K  ../test1.0.0
</span></span></code></pre></div><p>Nice. Our ZFS system compressed this 2 MiB file down to 165 KiB because it was generated with 100% compressibility setting.</p><p>Let&rsquo;s peek into the file to see what content was generated..</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@delorean:/sec_stor/backup/fiotest/fio-master <span class=c1># hexdump -C ../test1.0.0 |less</span>
</span></span><span class=line><span class=cl><span class=m>00000000</span>  de ad be ef de ad be ef  de ad be ef de ad be ef  <span class=p>|</span>................<span class=p>|</span>
</span></span><span class=line><span class=cl>*
</span></span><span class=line><span class=cl><span class=m>00200000</span>
</span></span><span class=line><span class=cl><span class=o>(</span>END<span class=o>)</span>
</span></span></code></pre></div><p>It&rsquo;s our requested pattern in the entire file because we asked for 100% compression.
Let&rsquo;s modify this test slightly to make it generate 0% compressible data..</p><p>Modifying the write_test.fio file with following changes,</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nv>buffer_compress_percentage</span><span class=o>=</span><span class=m>100</span>
</span></span><span class=line><span class=cl><span class=nv>buffer_pattern</span><span class=o>=</span>0xdeadbeef
</span></span></code></pre></div><p>to</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nv>buffer_compress_percentage</span><span class=o>=</span><span class=m>0</span>
</span></span><span class=line><span class=cl><span class=p>;</span><span class=nv>buffer_pattern</span><span class=o>=</span>0xdeadbeef <span class=p>;</span> this is commented out.
</span></span></code></pre></div><p>Repeat the run :</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@delorean:/sec_stor/backup/fiotest/fio-master <span class=c1># rm -rf ../test1.* ; sleep 1 ; ./fio write_test.fio</span>
</span></span><span class=line><span class=cl>test1: <span class=o>(</span><span class=nv>g</span><span class=o>=</span>0<span class=o>)</span>: <span class=nv>rw</span><span class=o>=</span>write, <span class=nv>bs</span><span class=o>=(</span>R<span class=o>)</span> 128KiB-128KiB, <span class=o>(</span>W<span class=o>)</span> 128KiB-128KiB, <span class=o>(</span>T<span class=o>)</span> 128KiB-128KiB, <span class=nv>ioengine</span><span class=o>=</span>psync, <span class=nv>iodepth</span><span class=o>=</span><span class=m>1</span>
</span></span><span class=line><span class=cl>fio-3.8
</span></span><span class=line><span class=cl>Starting <span class=m>1</span> process
</span></span><span class=line><span class=cl>~ ~ ~ ~ ~ ~ ~ ~ Timmed <span class=k>for</span> brevity.
</span></span></code></pre></div><p>check actual usage on disk,</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@delorean:/sec_stor/backup/fiotest/fio-master <span class=c1># du -sh ../test1.0.0</span>
</span></span><span class=line><span class=cl>2.0M  ../test1.0.0
</span></span></code></pre></div><p>It is using all of 2 MiB because it was not compressible as we expected.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>root@delorean:/sec_stor/backup/fiotest/fio-master <span class=c1># hexdump -C ../test1.0.0 | less</span>
</span></span><span class=line><span class=cl><span class=m>00000000</span>  a8 <span class=m>71</span> <span class=m>09</span> <span class=m>48</span> d3 ad 5f c5  <span class=m>35</span> 2e 2d b0 b5 <span class=m>51</span> 5a <span class=m>13</span>  <span class=p>|</span>.q.H.._.5.-..QZ.<span class=p>|</span>
</span></span><span class=line><span class=cl><span class=m>00000010</span>  c6 <span class=m>25</span> eb 1e <span class=m>20</span> <span class=m>72</span> c3 <span class=m>13</span>  b8 <span class=m>64</span> fa <span class=m>70</span> ce 5e <span class=m>52</span> <span class=m>18</span>  <span class=p>|</span>.%.. r...d.p.^R.<span class=p>|</span>
</span></span><span class=line><span class=cl><span class=m>00000020</span>  <span class=m>97</span> 4c c3 9b 5c <span class=m>13</span> ab <span class=m>06</span>  <span class=m>92</span> e9 2c ed <span class=m>89</span> <span class=m>14</span> <span class=m>88</span> <span class=m>15</span>  <span class=p>|</span>.L..<span class=se>\.</span>....,.....<span class=p>|</span>
</span></span><span class=line><span class=cl><span class=m>00000030</span>  <span class=m>32</span> 9d dc c8 fa 0b ea 1e  a6 <span class=m>93</span> <span class=m>82</span> 0a <span class=m>11</span> dd bd <span class=m>05</span>  <span class=p>|</span>2...............<span class=p>|</span>
</span></span><span class=line><span class=cl><span class=m>00000040</span>  <span class=m>74</span> <span class=m>52</span> 7d d7 <span class=m>60</span> <span class=m>36</span> <span class=m>39</span> 0a  4e aa b5 <span class=m>71</span> 0d bb <span class=m>42</span> 1a  <span class=p>|</span>tR<span class=o>}</span>.<span class=sb>`</span>69.N..q..B.<span class=p>|</span>
</span></span><span class=line><span class=cl><span class=m>00000050</span>  <span class=m>49</span> b5 0f d9 <span class=m>86</span> 9e <span class=m>63</span> <span class=m>12</span>  a9 <span class=m>76</span> 7d <span class=m>00</span> <span class=m>49</span> <span class=m>07</span> c8 <span class=m>09</span>  <span class=p>|</span>I.....c..v<span class=o>}</span>.I...<span class=p>|</span>
</span></span><span class=line><span class=cl>~ ~ ~ ~ ~ ~ ~ ~ Timmed <span class=k>for</span> brevity.
</span></span></code></pre></div><p>As expected, file is filled with random data which was not compressible.</p><p>Based on this setup, I set up a few tests by varying two parameters, compressibility and load(numfiles). Here is how my test matrix looks like..</p><div class=overflow-auto><table class="f6 w-100 mw8 center helvetica" cellspacing=0><thead><tr><th class="fw6 bb b--black-20 tl pb3 pr3 bg-white"></th><th class="fw6 bb b--black-20 tl pb3 pr3 bg-white">0% compressibility</th><th class="fw6 bb b--black-20 tl pb3 pr3 bg-white">50% compressibility</th><th class="fw6 bb b--black-20 tl pb3 pr3 bg-white">100% compressibility</th></tr></thead><tbody class=lh-copy><tr><td class="pv1 pr3 bb b--black-20">1 Proc<br>(1 X 128 GiB)<br>dataset size = 128 GiB</td><td class="pv1 pr3 bb b--black-20 tc">TBD</td><td class="pv1 pr3 bb b--black-20 tc">TBD</td><td class="pv1 pr3 bb b--black-20 tc">TBD</td></tr><tr><td class="pv1 pr3 bb b--black-20">2 Procs<br>(2 X 128 GiB)<br>dataset size = 256 GiB</td><td class="pv1 pr3 bb b--black-20 tc">TBD</td><td class="pv1 pr3 bb b--black-20 tc">TBD</td><td class="pv1 pr3 bb b--black-20 tc">TBD</td></tr><tr><td class="pv1 pr3 bb b--black-20">3 Proc<br>(3 X 128 GiB)<br>dataset size = 384 GiB</td><td class="pv1 pr3 bb b--black-20 tc">TBD</td><td class="pv1 pr3 bb b--black-20 tc">TBD</td><td class="pv1 pr3 bb b--black-20 tc">TBD</td></tr><tr><td class="pv1 pr3 bb b--black-20">. . . . . .</td><td class="pv1 pr3 bb b--black-20 tc">TBD</td><td class="pv1 pr3 bb b--black-20 tc">TBD</td><td class="pv1 pr3 bb b--black-20 tc">TBD</td></tr><tr><td class="pv1 pr3 bb b--black-20">9 Proc<br>(9 X 128 GiB)<br>dataset size = 896 GiB</td><td class="pv1 pr3 bb b--black-20 tc">TBD</td><td class="pv1 pr3 bb b--black-20 tc">TBD</td><td class="pv1 pr3 bb b--black-20 tc">TBD</td></tr></tbody></table></div><p>Note: data set set size increases in steps of 128 GiB along with the number of processes.</p><p>Keep in mind that my test system has 768 GiB of memory. So I tailored my test in a way that my dataset gets bigger than the total amount of memory at some point during the test.</p><div class="mw7-ns w-100"><script src=https://cdn.plot.ly/plotly-latest.min.js></script><div id=08fd11cf-2296-4698-8d6c-14c98530e550 style=height:100%;width:100% class=plotly-graph-div></div><script type=text/javascript>window.PLOTLYENV=window.PLOTLYENV||{},window.PLOTLYENV.BASE_URL="https://plot.ly",Plotly.newPlot("08fd11cf-2296-4698-8d6c-14c98530e550",[{type:"bar",x:[1623,2597,2954,2798,2528,2708,2625,2565,2674],y:["1 Proc <br> (1 X 128 GiB)","2 Proc <br> (2 X 128 GiB)","3 Proc <br> (3 X 128 GiB)","4 Proc <br> (4 X 128 GiB)","5 Proc <br> (5 X 128 GiB)","6 Proc <br> (6 X 128 GiB)","7 Proc <br> (7 X 128 GiB)","8 Proc <br> (8 X 128 GiB)","9 Proc <br> (9 X 128 GiB)"],marker:{color:"rgba(50, 171, 96, 0.6)",line:{color:"rgba(50, 171, 96, 1.0)",width:1}},name:"Average throuhput from duration of test in MiB/s (Higher is better)",orientation:"h",xaxis:"x1",yaxis:"y1"},{type:"scatter",x:[44.56,60.52,90.47,139,206.3,233.92,291.93,349.8,378.81],y:["1 Proc <br> (1 X 128 GiB)","2 Proc <br> (2 X 128 GiB)","3 Proc <br> (3 X 128 GiB)","4 Proc <br> (4 X 128 GiB)","5 Proc <br> (5 X 128 GiB)","6 Proc <br> (6 X 128 GiB)","7 Proc <br> (7 X 128 GiB)","8 Proc <br> (8 X 128 GiB)","9 Proc <br> (9 X 128 GiB)"],mode:"lines+markers",line:{color:"rgb(128, 0, 128)"},name:"Average completion latency in us (Lower is better)",xaxis:"x2",yaxis:"y2"}],{xaxis1:{domain:[0,.45],anchor:"y1"},yaxis1:{domain:[0,1],anchor:"x1"},xaxis2:{domain:[.47,1],anchor:"y2",zeroline:!1,showline:!1,showticklabels:!0,showgrid:!0,dtick:25e3,title:"<b>latency</b>"},yaxis2:{domain:[0,.85],anchor:"x2",showgrid:!1,showline:!0,showticklabels:!1,linecolor:"rgba(102, 102, 102, 0.8)",linewidth:2},title:"<b>FIO - sequential write performance at 0% compressibility</b>",yaxis:{showgrid:!1,showline:!1,showticklabels:!0,domain:[0,.85]},xaxis:{zeroline:!1,showline:!1,showticklabels:!0,showgrid:!0,domain:[0,.42],title:"<b>Avg. Throughput</b>"},legend:{x:.029,y:1.038,font:{size:10}},margin:{l:100,r:20,t:70,b:100},paper_bgcolor:"rgb(248, 248, 255)",plot_bgcolor:"rgb(248, 248, 255)",annotations:[{xref:"x2",yref:"y2",y:"1 Proc <br> (1 X 128 GiB)",x:25,text:"45.0us",font:{family:"Arial",size:12,color:"rgb(128, 0, 128)"},showarrow:!1},{xref:"x1",yref:"y1",y:"1 Proc <br> (1 X 128 GiB)",x:1626,text:"1623MiB/s",font:{family:"Arial",size:12,color:"rgb(4, 50, 124)"},showarrow:!1},{xref:"x2",yref:"y2",y:"2 Proc <br> (2 X 128 GiB)",x:41,text:"61.0us",font:{family:"Arial",size:12,color:"rgb(128, 0, 128)"},showarrow:!1},{xref:"x1",yref:"y1",y:"2 Proc <br> (2 X 128 GiB)",x:2600,text:"2597MiB/s",font:{family:"Arial",size:12,color:"rgb(4, 50, 124)"},showarrow:!1},{xref:"x2",yref:"y2",y:"3 Proc <br> (3 X 128 GiB)",x:70,text:"90.0us",font:{family:"Arial",size:12,color:"rgb(128, 0, 128)"},showarrow:!1},{xref:"x1",yref:"y1",y:"3 Proc <br> (3 X 128 GiB)",x:2957,text:"2954MiB/s",font:{family:"Arial",size:12,color:"rgb(4, 50, 124)"},showarrow:!1},{xref:"x2",yref:"y2",y:"4 Proc <br> (4 X 128 GiB)",x:119,text:"139.0us",font:{family:"Arial",size:12,color:"rgb(128, 0, 128)"},showarrow:!1},{xref:"x1",yref:"y1",y:"4 Proc <br> (4 X 128 GiB)",x:2801,text:"2798MiB/s",font:{family:"Arial",size:12,color:"rgb(4, 50, 124)"},showarrow:!1},{xref:"x2",yref:"y2",y:"5 Proc <br> (5 X 128 GiB)",x:186,text:"206.0us",font:{family:"Arial",size:12,color:"rgb(128, 0, 128)"},showarrow:!1},{xref:"x1",yref:"y1",y:"5 Proc <br> (5 X 128 GiB)",x:2531,text:"2528MiB/s",font:{family:"Arial",size:12,color:"rgb(4, 50, 124)"},showarrow:!1},{xref:"x2",yref:"y2",y:"6 Proc <br> (6 X 128 GiB)",x:214,text:"234.0us",font:{family:"Arial",size:12,color:"rgb(128, 0, 128)"},showarrow:!1},{xref:"x1",yref:"y1",y:"6 Proc <br> (6 X 128 GiB)",x:2711,text:"2708MiB/s",font:{family:"Arial",size:12,color:"rgb(4, 50, 124)"},showarrow:!1},{xref:"x2",yref:"y2",y:"7 Proc <br> (7 X 128 GiB)",x:272,text:"292.0us",font:{family:"Arial",size:12,color:"rgb(128, 0, 128)"},showarrow:!1},{xref:"x1",yref:"y1",y:"7 Proc <br> (7 X 128 GiB)",x:2628,text:"2625MiB/s",font:{family:"Arial",size:12,color:"rgb(4, 50, 124)"},showarrow:!1},{xref:"x2",yref:"y2",y:"8 Proc <br> (8 X 128 GiB)",x:330,text:"350.0us",font:{family:"Arial",size:12,color:"rgb(128, 0, 128)"},showarrow:!1},{xref:"x1",yref:"y1",y:"8 Proc <br> (8 X 128 GiB)",x:2568,text:"2565MiB/s",font:{family:"Arial",size:12,color:"rgb(4, 50, 124)"},showarrow:!1},{xref:"x2",yref:"y2",y:"9 Proc <br> (9 X 128 GiB)",x:359,text:"379.0us",font:{family:"Arial",size:12,color:"rgb(128, 0, 128)"},showarrow:!1},{xref:"x1",yref:"y1",y:"9 Proc <br> (9 X 128 GiB)",x:2677,text:"2674MiB/s",font:{family:"Arial",size:12,color:"rgb(4, 50, 124)"},showarrow:!1}]},{showLink:!1,linkText:"Export to plot.ly"})</script></div><p><figure><img src=/img/util_curve.png alt="Utilisation hockey stick curve" class="mx-auto my-0 rounded-md"></figure></p><p>What we see here is an example of what we call the hockey stick curve in performance engineering & queueing theory. Assuming constant service time, and constant arrival times, the queueing delay and hence response time follows this hockey stick curve. Once throughput hits a ceiling, the response times/latencies shoot up dramatically. The above chart is not as dramatic as the hockey stick because we&rsquo;re looking at averages of both throughput and latencies.</p><div class="mw7-ns w-100 pb2"><div id=dd45d8a2-94f8-4b26-addc-56f572e5307d style=height:100%;width:100% class=plotly-graph-div></div><script type=text/javascript>window.PLOTLYENV=window.PLOTLYENV||{},window.PLOTLYENV.BASE_URL="https://plot.ly",Plotly.newPlot("dd45d8a2-94f8-4b26-addc-56f572e5307d",[{type:"bar",x:[1757,2623,2050,2122,2388,2414,2210,2295,2332],y:["1 Proc <br> (1 X 128 GiB)","2 Proc <br> (2 X 128 GiB)","3 Proc <br> (3 X 128 GiB)","4 Proc <br> (4 X 128 GiB)","5 Proc <br> (5 X 128 GiB)","6 Proc <br> (6 X 128 GiB)","7 Proc <br> (7 X 128 GiB)","8 Proc <br> (8 X 128 GiB)","9 Proc <br> (9 X 128 GiB)"],marker:{color:"rgba(50, 171, 96, 0.6)",line:{color:"rgba(50, 171, 96, 1.0)",width:1}},name:"Average throuhput from duration of test in MiB/s (Higher is better)",orientation:"h",xaxis:"x1",yaxis:"y1"},{type:"scatter",x:[50.11,70.23,152.52,202.34,246.85,278.62,363.83,401.17,447.02],y:["1 Proc <br> (1 X 128 GiB)","2 Proc <br> (2 X 128 GiB)","3 Proc <br> (3 X 128 GiB)","4 Proc <br> (4 X 128 GiB)","5 Proc <br> (5 X 128 GiB)","6 Proc <br> (6 X 128 GiB)","7 Proc <br> (7 X 128 GiB)","8 Proc <br> (8 X 128 GiB)","9 Proc <br> (9 X 128 GiB)"],mode:"lines+markers",line:{color:"rgb(128, 0, 128)"},name:"Average completion latency in us (Lower is better)",xaxis:"x2",yaxis:"y2"}],{xaxis1:{domain:[0,.45],anchor:"y1"},yaxis1:{domain:[0,1],anchor:"x1"},xaxis2:{domain:[.47,1],anchor:"y2",zeroline:!1,showline:!1,showticklabels:!0,showgrid:!0,dtick:25e3,title:"<b>latency</b>"},yaxis2:{domain:[0,.85],anchor:"x2",showgrid:!1,showline:!0,showticklabels:!1,linecolor:"rgba(102, 102, 102, 0.8)",linewidth:2},title:"<b>FIO - sequential write performance at 50% compressibility</b>",yaxis:{showgrid:!1,showline:!1,showticklabels:!0,domain:[0,.85]},xaxis:{zeroline:!1,showline:!1,showticklabels:!0,showgrid:!0,domain:[0,.42],title:"<b>Avg. Throughput</b>"},legend:{x:.029,y:1.038,font:{size:10}},margin:{l:100,r:20,t:70,b:100},paper_bgcolor:"rgb(248, 248, 255)",plot_bgcolor:"rgb(248, 248, 255)",annotations:[{xref:"x2",yref:"y2",y:"1 Proc <br> (1 X 128 GiB)",x:30,text:"50.0us",font:{family:"Arial",size:12,color:"rgb(128, 0, 128)"},showarrow:!1},{xref:"x1",yref:"y1",y:"1 Proc <br> (1 X 128 GiB)",x:1760,text:"1757MiB/s",font:{family:"Arial",size:12,color:"rgb(4, 50, 124)"},showarrow:!1},{xref:"x2",yref:"y2",y:"2 Proc <br> (2 X 128 GiB)",x:50,text:"70.0us",font:{family:"Arial",size:12,color:"rgb(128, 0, 128)"},showarrow:!1},{xref:"x1",yref:"y1",y:"2 Proc <br> (2 X 128 GiB)",x:2626,text:"2623MiB/s",font:{family:"Arial",size:12,color:"rgb(4, 50, 124)"},showarrow:!1},{xref:"x2",yref:"y2",y:"3 Proc <br> (3 X 128 GiB)",x:133,text:"153.0us",font:{family:"Arial",size:12,color:"rgb(128, 0, 128)"},showarrow:!1},{xref:"x1",yref:"y1",y:"3 Proc <br> (3 X 128 GiB)",x:2053,text:"2050MiB/s",font:{family:"Arial",size:12,color:"rgb(4, 50, 124)"},showarrow:!1},{xref:"x2",yref:"y2",y:"4 Proc <br> (4 X 128 GiB)",x:182,text:"202.0us",font:{family:"Arial",size:12,color:"rgb(128, 0, 128)"},showarrow:!1},{xref:"x1",yref:"y1",y:"4 Proc <br> (4 X 128 GiB)",x:2125,text:"2122MiB/s",font:{family:"Arial",size:12,color:"rgb(4, 50, 124)"},showarrow:!1},{xref:"x2",yref:"y2",y:"5 Proc <br> (5 X 128 GiB)",x:227,text:"247.0us",font:{family:"Arial",size:12,color:"rgb(128, 0, 128)"},showarrow:!1},{xref:"x1",yref:"y1",y:"5 Proc <br> (5 X 128 GiB)",x:2391,text:"2388MiB/s",font:{family:"Arial",size:12,color:"rgb(4, 50, 124)"},showarrow:!1},{xref:"x2",yref:"y2",y:"6 Proc <br> (6 X 128 GiB)",x:259,text:"279.0us",font:{family:"Arial",size:12,color:"rgb(128, 0, 128)"},showarrow:!1},{xref:"x1",yref:"y1",y:"6 Proc <br> (6 X 128 GiB)",x:2417,text:"2414MiB/s",font:{family:"Arial",size:12,color:"rgb(4, 50, 124)"},showarrow:!1},{xref:"x2",yref:"y2",y:"7 Proc <br> (7 X 128 GiB)",x:344,text:"364.0us",font:{family:"Arial",size:12,color:"rgb(128, 0, 128)"},showarrow:!1},{xref:"x1",yref:"y1",y:"7 Proc <br> (7 X 128 GiB)",x:2213,text:"2210MiB/s",font:{family:"Arial",size:12,color:"rgb(4, 50, 124)"},showarrow:!1},{xref:"x2",yref:"y2",y:"8 Proc <br> (8 X 128 GiB)",x:381,text:"401.0us",font:{family:"Arial",size:12,color:"rgb(128, 0, 128)"},showarrow:!1},{xref:"x1",yref:"y1",y:"8 Proc <br> (8 X 128 GiB)",x:2298,text:"2295MiB/s",font:{family:"Arial",size:12,color:"rgb(4, 50, 124)"},showarrow:!1},{xref:"x2",yref:"y2",y:"9 Proc <br> (9 X 128 GiB)",x:427,text:"447.0us",font:{family:"Arial",size:12,color:"rgb(128, 0, 128)"},showarrow:!1},{xref:"x1",yref:"y1",y:"9 Proc <br> (9 X 128 GiB)",x:2335,text:"2332MiB/s",font:{family:"Arial",size:12,color:"rgb(4, 50, 124)"},showarrow:!1}]},{showLink:!1,linkText:"Export to plot.ly"})</script></div><p>and finally, fully compressible data..</p><div class="mw7-ns w-100 pb2"><div id=51f85019-3ca6-4333-a67a-a4c6b4282747 style=height:100%;width:100% class=plotly-graph-div></div><script type=text/javascript>window.PLOTLYENV=window.PLOTLYENV||{},window.PLOTLYENV.BASE_URL="https://plot.ly",Plotly.newPlot("51f85019-3ca6-4333-a67a-a4c6b4282747",[{type:"bar",x:[2491,4606,5950,6154,6548,6592,6510,6278,6378],y:["1 Proc <br> (1 X 128 GiB)","2 Proc <br> (2 X 128 GiB)","3 Proc <br> (3 X 128 GiB)","4 Proc <br> (4 X 128 GiB)","5 Proc <br> (5 X 128 GiB)","6 Proc <br> (6 X 128 GiB)","7 Proc <br> (7 X 128 GiB)","8 Proc <br> (8 X 128 GiB)","9 Proc <br> (9 X 128 GiB)"],marker:{color:"rgba(50, 171, 96, 0.6)",line:{color:"rgba(50, 171, 96, 1.0)",width:1}},name:"Average throuhput from duration of test in MiB/s (Higher is better)",orientation:"h",xaxis:"x1",yaxis:"y1"},{type:"scatter",x:[44,47,55,68,85,103,122,145,168],y:["1 Proc <br> (1 X 128 GiB)","2 Proc <br> (2 X 128 GiB)","3 Proc <br> (3 X 128 GiB)","4 Proc <br> (4 X 128 GiB)","5 Proc <br> (5 X 128 GiB)","6 Proc <br> (6 X 128 GiB)","7 Proc <br> (7 X 128 GiB)","8 Proc <br> (8 X 128 GiB)","9 Proc <br> (9 X 128 GiB)"],mode:"lines+markers",line:{color:"rgb(128, 0, 128)"},name:"Average completion latency in us (Lower is better)",xaxis:"x2",yaxis:"y2"}],{xaxis1:{domain:[0,.45],anchor:"y1"},yaxis1:{domain:[0,1],anchor:"x1"},xaxis2:{domain:[.47,1],anchor:"y2",zeroline:!1,showline:!1,showticklabels:!0,showgrid:!0,dtick:25e3,title:"<b>latency</b>"},yaxis2:{domain:[0,.85],anchor:"x2",showgrid:!1,showline:!0,showticklabels:!1,linecolor:"rgba(102, 102, 102, 0.8)",linewidth:2},title:"<b>FIO - sequential write performance at 100% compressibility</b>",yaxis:{showgrid:!1,showline:!1,showticklabels:!0,domain:[0,.85]},xaxis:{zeroline:!1,showline:!1,showticklabels:!0,showgrid:!0,domain:[0,.42],title:"<b>Avg. Throughput</b>"},legend:{x:.029,y:1.038,font:{size:10}},margin:{l:100,r:20,t:70,b:100},paper_bgcolor:"rgb(248, 248, 255)",plot_bgcolor:"rgb(248, 248, 255)",annotations:[{xref:"x2",yref:"y2",y:"1 Proc <br> (1 X 128 GiB)",x:24,text:"44.0us",font:{family:"Arial",size:12,color:"rgb(128, 0, 128)"},showarrow:!1},{xref:"x1",yref:"y1",y:"1 Proc <br> (1 X 128 GiB)",x:2494,text:"2491MiB/s",font:{family:"Arial",size:12,color:"rgb(4, 50, 124)"},showarrow:!1},{xref:"x2",yref:"y2",y:"2 Proc <br> (2 X 128 GiB)",x:27,text:"47.0us",font:{family:"Arial",size:12,color:"rgb(128, 0, 128)"},showarrow:!1},{xref:"x1",yref:"y1",y:"2 Proc <br> (2 X 128 GiB)",x:4609,text:"4606MiB/s",font:{family:"Arial",size:12,color:"rgb(4, 50, 124)"},showarrow:!1},{xref:"x2",yref:"y2",y:"3 Proc <br> (3 X 128 GiB)",x:35,text:"55.0us",font:{family:"Arial",size:12,color:"rgb(128, 0, 128)"},showarrow:!1},{xref:"x1",yref:"y1",y:"3 Proc <br> (3 X 128 GiB)",x:5953,text:"5950MiB/s",font:{family:"Arial",size:12,color:"rgb(4, 50, 124)"},showarrow:!1},{xref:"x2",yref:"y2",y:"4 Proc <br> (4 X 128 GiB)",x:48,text:"68.0us",font:{family:"Arial",size:12,color:"rgb(128, 0, 128)"},showarrow:!1},{xref:"x1",yref:"y1",y:"4 Proc <br> (4 X 128 GiB)",x:6157,text:"6154MiB/s",font:{family:"Arial",size:12,color:"rgb(4, 50, 124)"},showarrow:!1},{xref:"x2",yref:"y2",y:"5 Proc <br> (5 X 128 GiB)",x:65,text:"85.0us",font:{family:"Arial",size:12,color:"rgb(128, 0, 128)"},showarrow:!1},{xref:"x1",yref:"y1",y:"5 Proc <br> (5 X 128 GiB)",x:6551,text:"6548MiB/s",font:{family:"Arial",size:12,color:"rgb(4, 50, 124)"},showarrow:!1},{xref:"x2",yref:"y2",y:"6 Proc <br> (6 X 128 GiB)",x:83,text:"103.0us",font:{family:"Arial",size:12,color:"rgb(128, 0, 128)"},showarrow:!1},{xref:"x1",yref:"y1",y:"6 Proc <br> (6 X 128 GiB)",x:6595,text:"6592MiB/s",font:{family:"Arial",size:12,color:"rgb(4, 50, 124)"},showarrow:!1},{xref:"x2",yref:"y2",y:"7 Proc <br> (7 X 128 GiB)",x:102,text:"122.0us",font:{family:"Arial",size:12,color:"rgb(128, 0, 128)"},showarrow:!1},{xref:"x1",yref:"y1",y:"7 Proc <br> (7 X 128 GiB)",x:6513,text:"6510MiB/s",font:{family:"Arial",size:12,color:"rgb(4, 50, 124)"},showarrow:!1},{xref:"x2",yref:"y2",y:"8 Proc <br> (8 X 128 GiB)",x:125,text:"145.0us",font:{family:"Arial",size:12,color:"rgb(128, 0, 128)"},showarrow:!1},{xref:"x1",yref:"y1",y:"8 Proc <br> (8 X 128 GiB)",x:6281,text:"6278MiB/s",font:{family:"Arial",size:12,color:"rgb(4, 50, 124)"},showarrow:!1},{xref:"x2",yref:"y2",y:"9 Proc <br> (9 X 128 GiB)",x:148,text:"168.0us",font:{family:"Arial",size:12,color:"rgb(128, 0, 128)"},showarrow:!1},{xref:"x1",yref:"y1",y:"9 Proc <br> (9 X 128 GiB)",x:6381,text:"6378MiB/s",font:{family:"Arial",size:12,color:"rgb(4, 50, 124)"},showarrow:!1}]},{showLink:!1,linkText:"Export to plot.ly"})</script></div>Woohoo! if the data is highly compressible, ZFS munches it much faster because there are fewer disk writes. Infact, at peak throughput the disks were averaging only about 5 MiB/s.<p>Looking at the data together,</p><div class=overflow-auto><table class="f6 w-100 mw8 pt3 center helvetica" cellspacing=0><thead><tr><th class="fw6 bb b--black-20 tl pb3 pr3 bg-white"></th><th class="fw6 bb b--black-20 tl pb3 pr3 bg-white">0% compressibility</th><th class="fw6 bb b--black-20 tl pb3 pr3 bg-white">50% compressibility</th><th class="fw6 bb b--black-20 tl pb3 pr3 bg-white">100% compressibility</th></tr></thead><tbody class=lh-copy><tr><td class="pv1 pr3 bb b--black-20">1 Proc<br>(1 X 128 GiB)<br>dataset size = 128 GiB</td><td class="pv1 pr3 bb b--black-20 tc">1623 MiB/s @ 45 us</td><td class="pv1 pr3 bb b--black-20 tc">1756 MiB/s @ 50 us</td><td class="pv1 pr3 bb b--black-20 tc">2491 MiB/s @ 44 us</td></tr><tr><td class="pv1 pr3 bb b--black-20">2 Procs<br>(2 X 128 GiB)<br>dataset size = 256 GiB</td><td class="pv1 pr3 bb b--black-20 tc">2597 MiB/s @ 61 us</td><td class="pv1 pr3 bb b--black-20 tc">2623 MiB/s @ 70 us</td><td class="pv1 pr3 bb b--black-20 tc">4606 MiB/s @ 47 us</td></tr><tr><td class="pv1 pr3 bb b--black-20">3 Proc<br>(3 X 128 GiB)<br>dataset size = 384 GiB</td><td class="pv1 pr3 bb b--black-20 tc">2954 MiB/s @ 90 us</td><td class="pv1 pr3 bb b--black-20 tc">2050 MiB/s @ 153 us</td><td class="pv1 pr3 bb b--black-20 tc">5950 MiB/s @ 55 us</td></tr><tr><td class="pv1 pr3 bb b--black-20">4 Proc<br>(4 X 128 GiB)<br>dataset size = 512 GiB</td><td class="pv1 pr3 bb b--black-20 tc">2798 MiB/s @ 139 us</td><td class="pv1 pr3 bb b--black-20 tc">2122 MiB/s @ 202 us</td><td class="pv1 pr3 bb b--black-20 tc">6154 MiB/s @ 68 us</td></tr><tr><td class="pv1 pr3 bb b--black-20">5 Proc<br>(5 X 128 GiB)<br>dataset size = 640 GiB</td><td class="pv1 pr3 bb b--black-20 tc">2528 MiB/s @ 206 us</td><td class="pv1 pr3 bb b--black-20 tc">2388 MiB/s @ 247 us</td><td class="pv1 pr3 bb b--black-20 tc">6548 MiB/s @ 85 us</td></tr><tr><td class="pv1 pr3 bb b--black-20">6 Proc<br>(6 X 128 GiB)<br>dataset size = 768 GiB</td><td class="pv1 pr3 bb b--black-20 tc">2708 MiB/s @ 234 us</td><td class="pv1 pr3 bb b--black-20 tc">2414 MiB/s @ 279 us</td><td class="pv1 pr3 bb b--black-20 tc">6592 MiB/s @ 103 us</td></tr><tr><td class="pv1 pr3 bb b--black-20">7 Proc<br>(7 X 128 GiB)<br>dataset size = 896 GiB</td><td class="pv1 pr3 bb b--black-20 tc">2625 MiB/s @ 292 us</td><td class="pv1 pr3 bb b--black-20 tc">2210 MiB/s @ 364 us</td><td class="pv1 pr3 bb b--black-20 tc">6510 MiB/s @ 122 us</td></tr><tr><td class="pv1 pr3 bb b--black-20">8 Proc<br>(8 X 128 GiB)<br>dataset size = 1024 GiB</td><td class="pv1 pr3 bb b--black-20 tc">2565 MiB/s @ 350 us</td><td class="pv1 pr3 bb b--black-20 tc">2295 MiB/s @ 401 us</td><td class="pv1 pr3 bb b--black-20 tc">6278 MiB/s @ 145 us</td></tr><tr><td class="pv1 pr3 bb b--black-20">9 Proc<br>(9 X 128 GiB)<br>dataset size = 1152 GiB</td><td class="pv1 pr3 bb b--black-20 tc">2674 MiB/s @ 379 us</td><td class="pv1 pr3 bb b--black-20 tc">2332 MiB/s @ 447 us</td><td class="pv1 pr3 bb b--black-20 tc">6348 MiB/s @ 168 us</td></tr></tbody></table></div><p>Out of curiosity, I took a look at the performance of the system while the IO test was in progress. Here are some pleasant sights I had.</p><p>Overall disk bandwidth slightly less than 3 GB per second!<figure><img src=/img/zpool_iostat.png alt="zpool iostat showing disk util" class="mx-auto my-0 rounded-md"></figure></p><p>To put this in perspective, the network interface on this server is a 10Gbps link.</p><p><code>10Gbps</code> = <code>1250 MB/s</code> or <code>1192 MiB/s</code>.</p><blockquote class="athelas ml0 mt0 pl4 black-90 bl bw2 b--blue"><div class="f5 f4-m f3-l i lh-copy measure mt0">Our backup server is servicing writes faster than the 10 Gbps network it is connected to!</div></blockquote><p>While this is happening, here is htop..<figure><img src=/img/htop_zfs.png alt="htop view" class="mx-auto my-0 rounded-md"></figure></p><p>Good that the fio processes are not CPU bound.</p><p>Here is another view of individual disk&rsquo;s util through systat..</p><p><figure><img src=/img/systat.png alt="systat view of ZFS" class="mx-auto my-0 rounded-md"></figure></p><p>Ok, so each of these disks are doing approximately 30 MiB/s. However, the manufacturer rating of these disks are 237 MiB/s..</p><p><figure><img src=/img/disk_rating_perf.png alt="disk rating from manufacturer" class="mx-auto my-0 rounded-md"></figure></p><p>Deriving performance out of disks is more complicated than this. The manufacturer ratings dont apply for all conditions in which the IO hits the disks. IO sizes often play a big role in determining max throughput from a disk. Another factor is caching infront of the disks, and in the storage drivers. As an example of such wild swings, here is a snip from Matthew Rocklin&rsquo;s research into disk throughput vs file size.</p><p><figure><img src=/img/disk-bandwidth-by-file-size.png alt="disk bw by file size.." class="mx-auto my-0 rounded-md"></figure></p><p>There may be opportunities to remove bottlenecks and further improve performance. But, that would be useless when the 10Gbps network is already a bottleneck.</p><h3 id=conclusion class="relative group">Conclusion <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#conclusion aria-label=Anchor>#</a></span></h3><p>It was fun looking at the performance of the ZFS server in the context it will be used at. I&rsquo;m amazed particularly by how ZFS handle compressible data with ease. At some point it should become the default. Knowing that the system I built exceeded performance goals is always good. Hopefully, these notes above helps others tailor their test cases to anlyze different scenarios.</p></div></section><footer class="max-w-prose pt-8 print:hidden"><div class=flex><div class=place-self-center><div class="text-2xl sm:text-lg"></div></div></div><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="group flex" href=/posts/file-server-freebsd-zfs/><span class="me-2 text-neutral-700 transition-transform group-hover:-translate-x-[2px] group-hover:text-primary-600 dark:text-neutral dark:group-hover:text-primary-400"><span class="ltr:inline rtl:hidden">&larr;</span><span class="ltr:hidden rtl:inline">&rarr;</span></span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Building a file server with FreeBSD, ZFS and a lot of disks</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime="2018-07-16 12:00:00 +0000 UTC">16 July 2018</time>
</span></span></a></span><span><a class="group flex text-right" href=/posts/investments-2024/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">How and what I invest in 2024</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime="2024-01-12 12:00:00 +0000 UTC">12 January 2024</time>
</span></span><span class="ms-2 text-neutral-700 transition-transform group-hover:-translate-x-[-2px] group-hover:text-primary-600 dark:text-neutral dark:group-hover:text-primary-400"><span class="ltr:inline rtl:hidden">&rarr;</span><span class="ltr:hidden rtl:inline">&larr;</span></span></a></span></div></div></footer></article></main><div class="pointer-events-none absolute bottom-0 end-0 top-[100vh] w-12" id=to-top hidden=true><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 backdrop-blur hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div><footer class="py-10 print:hidden"><div class="flex items-center justify-between"><div><p class="text-sm text-neutral-500 dark:text-neutral-400">Made with  by a human.</p></div><div class="flex flex-row items-center"></div></div></footer><div id=search-wrapper class="invisible fixed inset-0 z-50 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://aravindh.net/><div id=search-modal class="top-20 mx-auto flex min-h-0 w-full max-w-3xl flex-col rounded-md border border-neutral-200 bg-neutral shadow-lg dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex flex-none items-center justify-between px-2"><form class="flex min-w-0 flex-auto items-center"><div class="flex h-8 w-8 items-center justify-center text-neutral-400"><span class="icon relative inline-block px-1 align-text-bottom"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="mx-1 flex h-12 flex-auto appearance-none bg-transparent focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex h-8 w-8 items-center justify-center text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="icon relative inline-block px-1 align-text-bottom"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto overflow-auto px-2"><ul id=search-results></ul></section></div></div></div></body></html>